{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.505 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from flyai.dataset import Dataset\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Conv1D, Concatenate, BatchNormalization\n",
    "\n",
    "import create_dict\n",
    "import processor\n",
    "from attention import AttLayer\n",
    "from model import Model\n",
    "from path import DATA_PATH\n",
    "from path import MODEL_PATH\n",
    "\n",
    "#sys.stdout = codecs.getwriter(\"utf-8\")(sys.stdout.detach())\n",
    "\n",
    "rnn_unit_1 = 100  # 第一层lstm包含cell个数\n",
    "rnn_unit_2 = 100  # 第二层lstm包含cell个数\n",
    "conv_dim = 128\n",
    "embed_dim = 200\n",
    "class_num = 2\n",
    "\n",
    "\n",
    "def load_embed(char_dict):\n",
    "    embed_path = os.path.join(DATA_PATH, 'embedding.json')\n",
    "    with open(embed_path, encoding='utf-8') as jsonin:\n",
    "        embed = json.load(jsonin)\n",
    "    MAX_CHAR = max(char_dict.values())\n",
    "    embed_mat = np.zeros((MAX_CHAR + 1, embed_dim))\n",
    "    for c, v in embed.items():\n",
    "        if c in char_dict and v != 'nan':\n",
    "            embed_mat[char_dict[c], :] = v\n",
    "    return embed_mat\n",
    "\n",
    "\n",
    "# 数据获取辅助类\n",
    "dataset = Dataset()\n",
    "# 模型操作辅助类\n",
    "model = Model(dataset)\n",
    "# 超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[  320,   127,     4, ...,     0,     0,     0],\n",
       "         [ 3980,  4787,    18, ...,     0,     0,     0],\n",
       "         [26182,     0,     0, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 3320, 10543,  4787, ...,     0,     0,     0],\n",
       "         [21806,  2880,     0, ...,     0,     0,     0],\n",
       "         [ 4650, 29533,     0, ...,     0,     0,     0]]),\n",
       "  array([[ 320,  127,    4, ...,    0,    0,    0],\n",
       "         [1482, 3984,  223, ...,    0,    0,    0],\n",
       "         [4939, 3920,    0, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [1562,  128, 1330, ...,    0,    0,    0],\n",
       "         [1238, 1106,  196, ...,    0,    0,    0],\n",
       "         [2362, 2237,  420, ...,    0,    0,    0]])],\n",
       " array([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]),\n",
       " [array([[  320,   127,     4, ...,     0,     0,     0],\n",
       "         [ 3980,  4787,    18, ...,     0,     0,     0],\n",
       "         [26182,     0,     0, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [ 3320, 10543,  4787, ...,     0,     0,     0],\n",
       "         [21806,  2880,     0, ...,     0,     0,     0],\n",
       "         [ 4650, 29533,     0, ...,     0,     0,     0]]),\n",
       "  array([[ 320,  127,    4, ...,    0,    0,    0],\n",
       "         [1482, 3984,  223, ...,    0,    0,    0],\n",
       "         [4939, 3920,    0, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [1562,  128, 1330, ...,    0,    0,    0],\n",
       "         [1238, 1106,  196, ...,    0,    0,    0],\n",
       "         [2362, 2237,  420, ...,    0,    0,    0]])],\n",
       " array([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_all_processor_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "from flyai.processor.base import Base\n",
    "\n",
    "from path import DATA_PATH  # 导入输入数据的地址\n",
    "\n",
    "jieba.load_userdict(os.path.join(DATA_PATH, 'keywords'))\n",
    "import numpy as np\n",
    "import create_dict\n",
    "\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, word_dict_res = create_dict.load_dict()\n",
    "\n",
    "text=\"我今晚上能见到他吗？\"\n",
    "if type(text) is not str:\n",
    "    with open('data/err_text.txt', 'a', encoding='utf-8') as fout:\n",
    "        fout.write('{}\\n'.format(text))\n",
    "    text = str(text)\n",
    "terms = jieba.cut(text, cut_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tokenizer.cut at 0x7f7366927518>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_terms = []\n",
    "for term in terms:\n",
    "    print(term)\n",
    "    truncate_terms.append(term)\n",
    "    if len(truncate_terms) >= MAX_LEN:\n",
    "        break\n",
    "index_list = [word_dict[term] if term in word_dict\n",
    "              else create_dict._UNK_ for term in truncate_terms]\n",
    "if len(index_list) < MAX_LEN:\n",
    "    index_list = index_list + [create_dict._PAD_] * (MAX_LEN - len(index_list))\n",
    "\n",
    "char_index_list = [word_dict[c] if c in word_dict\n",
    "                   else create_dict._UNK_ for c in text]\n",
    "char_index_list = char_index_list[:MAX_LEN]\n",
    "if len(char_index_list) < MAX_LEN:\n",
    "    char_index_list = char_index_list + [create_dict._PAD_] * (MAX_LEN - len(char_index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--EPOCHS\", default=10, type=int, help=\"train epochs\")\n",
    "parser.add_argument(\"-b\", \"--BATCH\", default=32, type=int, help=\"batch size\")\n",
    "args = parser.parse_args([])\n",
    "MAX_LEN = processor.MAX_LEN\n",
    "\n",
    "word_dict, word_dict_res = create_dict.load_dict()\n",
    "num_word = max(word_dict.values()) + 1\n",
    "# ——————————————————导入数据——————————————————————\n",
    "embed_mat = load_embed(word_dict)\n",
    "input_x = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "input_xc = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "\n",
    "\n",
    "def model_400M():\n",
    "    def feed_input(input_x, sub_name):\n",
    "        x1 = Embedding(input_dim=num_word, output_dim=embed_dim, name=sub_name + 'embed_s',\n",
    "                       weights=[embed_mat], trainable=False)(input_x)\n",
    "        x2 = Embedding(input_dim=num_word, output_dim=embed_dim, name=sub_name + 'embed_d',\n",
    "                       weights=[embed_mat], trainable=True)(input_x)\n",
    "        x = Concatenate()([x1, x2])\n",
    "        # CNN model\n",
    "        kls = [2, 3, 4, 5]\n",
    "        hs = []\n",
    "        for kl in kls:\n",
    "            h = Conv1D(conv_dim, kl, activation='relu')(x)\n",
    "            # h = GlobalMaxPool1D()(h)\n",
    "            h = AttLayer()(h)\n",
    "            hs.append(h)\n",
    "        h2 = Concatenate()(hs)\n",
    "        h2 = BatchNormalization()(h2)\n",
    "        return h2\n",
    "\n",
    "    h2 = feed_input(input_x, 'term')\n",
    "    h2c = feed_input(input_xc, 'char')\n",
    "    h2 = Concatenate()([h2, h2c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集合大小为315364，大约1000*256\n",
    "embed = Embedding(input_dim=num_word, output_dim=embed_dim, name='embed_s',\n",
    "                  weights=[embed_mat], trainable=True)\n",
    "x = embed(input_x)\n",
    "h2_term = LSTM(rnn_unit_1, return_sequences=True)(x)\n",
    "h2_term = AttLayer()(h2_term)\n",
    "x_c = embed(input_xc)\n",
    "h2_c = LSTM(rnn_unit_1, return_sequences=True)(x_c)\n",
    "h2_c = AttLayer()(h2_c)\n",
    "h2 = Concatenate()([h2_term, h2_c])\n",
    "\n",
    "pred = Dense(class_num, activation='softmax')(h2)\n",
    "k_model = keras.Model([input_x, input_xc], pred)\n",
    "opt = keras.optimizers.Adam(0.001)\n",
    "k_model.compile(opt, 'categorical_crossentropy', ['acc', ])\n",
    "\n",
    "earlystop = EarlyStopping(min_delta=0.01, patience=1)\n",
    "save_best = ModelCheckpoint(os.path.join(MODEL_PATH, \"model.h5\"), save_best_only=True)\n",
    "\n",
    "save_epochs = 100\n",
    "best_loss = 1e6\n",
    "patient = 2\n",
    "patient_count = patient\n",
    "for epochs in range(args.EPOCHS):\n",
    "    if epochs == args.EPOCHS - 1 or (epochs != 0 and epochs % save_epochs == 0):\n",
    "        x_train, y_train, x_test, y_test = dataset.next_batch(args.BATCH, test_data=True)\n",
    "        k_model.fit(x_train, y_train, batch_size=args.BATCH, verbose=1)\n",
    "        score = k_model.evaluate(x_test, y_test, verbose=1, batch_size=args.BATCH)\n",
    "        print('val score', score)\n",
    "        score = score[0]  # val loss\n",
    "        if score <= best_loss:\n",
    "            model.save_model(k_model, MODEL_PATH, name='model.h5', overwrite=True)\n",
    "            patient_count = patient\n",
    "        else:\n",
    "            patient_count -= 1\n",
    "        if patient_count < 0:\n",
    "            break\n",
    "    else:\n",
    "        x_train, y_train, x_test, y_test = dataset.next_batch(args.BATCH, test_data=False)\n",
    "        verbose = 1 if epochs % 10 == 0 else 0\n",
    "        k_model.fit(x_train, y_train, batch_size=args.BATCH, verbose=verbose)\n",
    "    if epochs % 10 == 0:\n",
    "        print(str(epochs) + \"/\" + str(args.EPOCHS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
