{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.settings {'net': 'densenet121', 'nc': 2, 'lr': 0.001, 'seed': 1, 'log_interval': 100, 'save_model': True}\n",
      "{'id': 'ReadingComprehesionCh', 'type': 'csv', 'config': {'train_url': 'https://dataset.flyai.com/dataset/ReadingComprehesionCh/dev.zip', 'test_url': 'https://dataset.flyai.com/dataset/ReadingComprehesionCh/dev.zip'}}\n",
      "[200.0, 0.03, 0.1710152950930964, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "Barnes & Noble                                                                                                                                         2\n",
      "营养素含量                                                                                                                                                  2\n",
      "地处山麓台地与沿海平原交接地带，为瀑布线城市之一。                                                                                                                              2\n",
      "吻部坚硬锋利。                                                                                                                                                2\n",
      "我想我们之所以会在激烈的竞争中打败好莱坞，是因为康斯坦丁公司是独立制片公司，当我们作出决定，计划便很快得以实施，不必征求上百位决策人的同意。                                                                                 2\n",
      "花果期7月。                                                                                                                                                 2\n",
      "2012年4月24日迅海计划决标，由台湾民营造船业者龙德造船工业股份有限公司得标，中华民国国防部规划从2012到2014年，投入新台币20多亿元，先建造一艘迅海级原型舰进行测试，等构型确定后即可量产；最后，中华民国海军希望能布署十多艘的迅海舰，与光华六号导弹快艇混合编组，执行反舰和反船团任务。    2\n",
      "当然，你可能会受到各种抗性的制约，但是无论如何，这把剑绝对是单手武器中的神器！                                                                                                                2\n",
      "馨巧队：李馨巧、孙坚、贾玲、黄艺馨、刘维                                                                                                                                   2\n",
      "泉边芦苇茂密，微风起伏，碧波荡漾，水映沙山，颇为奇观。                                                                                                                            2\n",
      "Name: answer, dtype: int64\n",
      "0    194\n",
      "1      6\n",
      "Name: label, dtype: int64\n",
      "16 12.5\n",
      "2. load data. train_dataset 200,batch 12, val_dataset 100, batch 6.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "# author ChangeBio\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from flyai.dataset import Dataset\n",
    "from flyai.source.base import DATA_PATH\n",
    "\n",
    "from model import Model\n",
    "from net import Net, LSTMNet\n",
    "from path import MODEL_PATH\n",
    "from processor import DQuestionFlyAI,load_data, make_weights_for_balanced_classes\n",
    "from utils import Bunch\n",
    "\n",
    "def eval(model, x_test, y_test):\n",
    "    network.eval()\n",
    "    total_acc = 0.0\n",
    "    data_len = len(x_test[0])\n",
    "    x1, x2 = x_test\n",
    "    x1 = torch.from_numpy(x1)\n",
    "    x2 = torch.from_numpy(x2)\n",
    "    x1 = x1.float().to(device)\n",
    "    x2 = x2.float().to(device)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    y_test = y_test.to(device)\n",
    "    batch_eval = model.batch_iter(x1, x2, y_test)\n",
    "\n",
    "    for x_batch1, x_batch2, y_batch in batch_eval:\n",
    "        outputs = network(x_batch1, x_batch2)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        correct = (prediction == y_batch).sum().item()\n",
    "        total_acc += correct\n",
    "    return total_acc / data_len\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--EPOCHS\", default=10, type=int, help=\"train epochs\")\n",
    "parser.add_argument(\"-b\", \"--BATCH\", default=16, type=int, help=\"batch size\")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "#settings\n",
    "settings = {\n",
    "'net':'densenet121',\n",
    "'nc':2,    \n",
    "'lr': 0.001,\n",
    "'seed': 1,\n",
    "'log_interval': 100,\n",
    "'save_model': True}\n",
    "print(\"1.settings\",settings)\n",
    "settings = Bunch(settings)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(settings.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "#load data\n",
    "#data = Dataset()\n",
    "#model = Model(data)\n",
    "trn,val = load_data()\n",
    "train_dataset = DQuestionFlyAI(root=DATA_PATH,df=trn)\n",
    "val_dataset = DQuestionFlyAI(root=DATA_PATH,df=val)\n",
    "weight = make_weights_for_balanced_classes(trn.label,settings.nc)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weight,len(weight))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=args.BATCH, sampler =sampler, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "    batch_size=args.BATCH, **kwargs)\n",
    "n_train = len(train_dataset)\n",
    "batch_train = n_train/args.BATCH\n",
    "print(args.BATCH,n_train/args.BATCH)\n",
    "n_test = len(val_dataset)\n",
    "batch_test = n_test/args.BATCH\n",
    "print(\"2. load data. train_dataset %d,batch %d, val_dataset %d, batch %d.\" % (n_train,batch_train,n_test,batch_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2, y_train = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [train_dataset[i] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.464023, -0.055275, -0.358562, ..., -0.638851, -0.109829,\n",
       "          0.397766],\n",
       "        [ 0.325058, -0.380674,  0.468678, ..., -0.647425, -0.230343,\n",
       "          0.366162],\n",
       "        [ 0.333389,  0.254132,  0.381571, ..., -0.035394, -0.017113,\n",
       "          0.322221],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]]),\n",
       " array([[ 0.223334, -0.032572,  0.12772 , ..., -0.134228, -0.200968,\n",
       "         -0.089978],\n",
       "        [-0.134102, -0.393164, -0.030239, ..., -0.221924, -0.166869,\n",
       "         -0.088888],\n",
       "        [ 0.209092, -0.165459, -0.058054, ..., -0.278624, -0.173408,\n",
       "          0.035439],\n",
       "        ...,\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ]]),\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.load net structure: densenet121, number of class: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.optimize and loss. learning rate 0.001\n",
      "5.***************train and test*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.473 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-46a5213228c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#load net structure\n",
    "print(\"3.load net structure: %s, number of class: %d\" % (settings.net,settings.nc))\n",
    "net = LSTMNet()\n",
    "gpu = torch.cuda.is_available()\n",
    "if gpu:\n",
    "    net.cuda()\n",
    "#optimize and loss\n",
    "print(\"4.optimize and loss. learning rate %g\" % settings.lr)\n",
    "optimizer = Adam(net.parameters(), lr=settings.lr, weight_decay=1e-4)\n",
    "#optimizer = Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))  \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "#train and test\n",
    "print(\"5.***************train and test*********************\")\n",
    "best_accuracy = 0\n",
    "train_iter = iter(train_loader)\n",
    "batch_idx = 0\n",
    "for epoch in range(args.EPOCHS):\n",
    "    net.train()\n",
    "    #x_train, y_train, x_test, y_test = data.next_batch(args.BATCH)  # 读取数据\n",
    "    #batch_len = y_train.shape[0]\n",
    "    #x1, x2 = x_train\n",
    "    #x1 = torch.from_numpy(x1)\n",
    "    #x2 = torch.from_numpy(x2)\n",
    "    #x1 = x1.float().to(device)\n",
    "    #x2 = x2.float().to(device)\n",
    "    #y_train = torch.from_numpy(y_train)\n",
    "    #y_train = y_train.to(device)\n",
    "    try:\n",
    "        batch_idx +=1\n",
    "        x1,x2, y_train = next(train_iter)\n",
    "        #print(batch_idx,\"data len\",len(x_train),len(y_train))\n",
    "    except:\n",
    "        batch_idx = 0\n",
    "        train_iter = iter(train_loader)\n",
    "        x1,x2, y_train = next(train_iter)\n",
    "        print(len(x1),len(x2),len(y_train),\"data len\")\n",
    "\n",
    "    if gpu:\n",
    "        x1 = Variable(x1.cuda())\n",
    "        x2 = Variable(x2.cuda())\n",
    "        y_train = Variable(y_train.long().cuda())\n",
    "\n",
    "\n",
    "    outputs = net(x1,x2)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % settings.log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(y_train), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "if settings.save_model:       \n",
    "    model.save_model(net, MODEL_PATH, overwrite=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
