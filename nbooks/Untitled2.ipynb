{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneCycleScheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-76f6fd7cbea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m  'fit_one_cycle', 'lr_find', 'one_cycle_scheduler', 'to_fp16', 'to_fp32', 'mixup', 'AccumulateScheduler']\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mone_cycle_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mOneCycleScheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"Instantiate a `OneCycleScheduler` with `lr_max`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOneCycleScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OneCycleScheduler' is not defined"
     ]
    }
   ],
   "source": [
    "\"Provides advanced training extensions to `fastai.basic_train`. Includes half-precision, learning rate finder, mixup, and one-cycle\"\n",
    "from torch_core import *\n",
    "#from callbacks import *\n",
    "from basic_data import *\n",
    "from basic_train import *\n",
    "\n",
    "__all__ = ['BnFreeze', 'GradientClipping', 'ShowGraph', 'Interpretation', 'ClassificationInterpretation', 'MultiLabelClassificationInterpretation',\n",
    " 'fit_one_cycle', 'lr_find', 'one_cycle_scheduler', 'to_fp16', 'to_fp32', 'mixup', 'AccumulateScheduler']\n",
    "\n",
    "def one_cycle_scheduler(lr_max:float, **kwargs:Any)->OneCycleScheduler:\n",
    "    \"Instantiate a `OneCycleScheduler` with `lr_max`.\"\n",
    "    return partial(OneCycleScheduler, lr_max=lr_max, **kwargs)\n",
    "\n",
    "def fit_one_cycle(learn:Learner, cyc_len:int, max_lr:Union[Floats,slice]=defaults.lr,\n",
    "                  moms:Tuple[float,float]=(0.95,0.85), div_factor:float=25., pct_start:float=0.3, final_div:float=None,\n",
    "                  wd:float=None, callbacks:Optional[CallbackList]=None, tot_epochs:int=None, start_epoch:int=None)->None:\n",
    "    \"Fit a model following the 1cycle policy.\"\n",
    "    max_lr = learn.lr_range(max_lr)\n",
    "    callbacks = listify(callbacks)\n",
    "    callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n",
    "                                       final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n",
    "    learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks)\n",
    "\n",
    "def lr_find(learn:Learner, start_lr:Floats=1e-7, end_lr:Floats=10, num_it:int=100, stop_div:bool=True, wd:float=None):\n",
    "    \"Explore lr from `start_lr` to `end_lr` over `num_it` iterations in `learn`. If `stop_div`, stops when loss diverges.\"\n",
    "    start_lr = learn.lr_range(start_lr)\n",
    "    start_lr = np.array(start_lr) if is_listy(start_lr) else start_lr\n",
    "    end_lr = learn.lr_range(end_lr)\n",
    "    end_lr = np.array(end_lr) if is_listy(end_lr) else end_lr\n",
    "    cb = LRFinder(learn, start_lr, end_lr, num_it, stop_div)\n",
    "    epochs = int(np.ceil(num_it/len(learn.data.train_dl)))\n",
    "    learn.fit(epochs, start_lr, callbacks=[cb], wd=wd)\n",
    "\n",
    "def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
    "            flat_master:bool=False, max_scale:float=2**24)->Learner:\n",
    "    \"Put `learn` in FP16 precision mode.\"\n",
    "    learn.to_fp32()\n",
    "    learn.model = model2half(learn.model)\n",
    "    learn.data.add_tfm(batch_to_half)\n",
    "    learn.mp_cb = MixedPrecision(learn, loss_scale=loss_scale, max_noskip=max_noskip, dynamic=dynamic, clip=clip, \n",
    "                                 flat_master=flat_master, max_scale=max_scale)\n",
    "    learn.callbacks.append(learn.mp_cb)\n",
    "    return learn\n",
    "\n",
    "def to_fp32(learn:Learner):\n",
    "    \"Put `learn` back to FP32 precision mode.\"\n",
    "    learn.data.remove_tfm(batch_to_half)\n",
    "    for cb in learn.callbacks: \n",
    "        if isinstance(cb, MixedPrecision): learn.callbacks.remove(cb)\n",
    "    learn.model = learn.model.float()\n",
    "    return learn\n",
    "\n",
    "def mixup(learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True) -> Learner:\n",
    "    \"Add mixup https://arxiv.org/abs/1710.09412 to `learn`.\"\n",
    "    learn.callback_fns.append(partial(MixUpCallback, alpha=alpha, stack_x=stack_x, stack_y=stack_y))\n",
    "    return learn\n",
    "\n",
    "Learner.fit_one_cycle = fit_one_cycle\n",
    "Learner.lr_find = lr_find\n",
    "Learner.to_fp16 = to_fp16\n",
    "Learner.to_fp32 = to_fp32\n",
    "Learner.mixup = mixup\n",
    "\n",
    "class ShowGraph(LearnerCallback):\n",
    "    \"Update a graph of learner stats and metrics after each epoch.\"\n",
    "    def on_epoch_end(self, n_epochs:int, last_metrics:MetricsList, **kwargs)->bool:\n",
    "        \"If we have `last_metrics` plot them in our pbar graph\"\n",
    "        if last_metrics is not None and np.any(last_metrics):\n",
    "            rec = self.learn.recorder\n",
    "            iters = range_of(rec.losses)\n",
    "            val_iter = np.array(rec.nb_batches).cumsum()\n",
    "            x_bounds = (0, (n_epochs - len(rec.nb_batches)) * rec.nb_batches[-1] + len(rec.losses))\n",
    "            y_bounds = (0, max((max(Tensor(rec.losses)), max(Tensor(rec.val_losses)))))\n",
    "            rec.pbar.update_graph([(iters, rec.losses), (val_iter, rec.val_losses)], x_bounds, y_bounds)\n",
    "        return {}\n",
    "\n",
    "class BnFreeze(LearnerCallback):\n",
    "    \"Freeze moving average statistics in all non-trainable batchnorm layers.\"\n",
    "    def on_epoch_begin(self, **kwargs:Any)->None:\n",
    "        \"Put bn layers in eval mode just after `model.train()`.\"\n",
    "        set_bn_eval(self.learn.model)\n",
    "\n",
    "class GradientClipping(LearnerCallback):\n",
    "    \"Gradient clipping during training.\"\n",
    "    def __init__(self, learn:Learner, clip:float = 0.):\n",
    "        super().__init__(learn)\n",
    "        self.clip = clip\n",
    "\n",
    "    def on_backward_end(self, **kwargs):\n",
    "        \"Clip the gradient before the optimizer step.\"\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)\n",
    "\n",
    "def clip_grad(learn:Learner, clip:float=0.1)->Learner:\n",
    "    \"Add gradient clipping of `clip` during training.\"\n",
    "    learn.callback_fns.append(partial(GradientClipping, clip=clip))\n",
    "    return learn\n",
    "Learner.clip_grad = clip_grad\n",
    "     \n",
    "class AccumulateScheduler(LearnerCallback):\n",
    "    \"Does accumlated step every nth step by accumulating gradients\"\n",
    "    \n",
    "    def __init__(self, learn:Learner, n_step:int = 1, drop_last:bool = False):\n",
    "        super().__init__(learn)\n",
    "        self.n_step,self.drop_last = n_step,drop_last\n",
    " \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"check if loss is reduction\"\n",
    "        if hasattr(self.loss_func, \"reduction\") and (self.loss_func.reduction != \"sum\"):\n",
    "             warn(\"For better gradients consider 'reduction=sum'\")\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        \"init samples and batches, change optimizer\"\n",
    "        self.acc_samples, self.acc_batches = 0., 0. \n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        \"accumulate samples and batches\"\n",
    "        self.acc_samples += last_input.shape[0]\n",
    "        self.acc_batches += 1\n",
    "        \n",
    "    def on_backward_end(self, **kwargs):\n",
    "        \"accumulated step and reset samples, True will result in no stepping\"\n",
    "        if (self.acc_batches % self.n_step) == 0:\n",
    "            for p in (self.learn.model.parameters()):\n",
    "                if p.requires_grad: p.grad.div_(self.acc_samples)\n",
    "            self.acc_samples = 0\n",
    "        else: return {'skip_step':True, 'skip_zero':True}\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        \"step the rest of the accumulated grads if not perfectly divisible\"\n",
    "        for p in (self.learn.model.parameters()):\n",
    "                if p.requires_grad: p.grad.div_(self.acc_samples)\n",
    "        if not self.drop_last: self.learn.opt.step()\n",
    "        self.learn.opt.zero_grad()\n",
    "\n",
    "\n",
    "class Interpretation():\n",
    "    \"Interpretation base class, can be inherited for task specific Interpretation classes\"\n",
    "    def __init__(self, learn:Learner, preds:Tensor, y_true:Tensor, losses:Tensor, ds_type:DatasetType=DatasetType.Valid):\n",
    "        self.data,self.preds,self.y_true,self.losses,self.ds_type, self.learn = \\\n",
    "                                 learn.data,preds,y_true,losses,ds_type,learn\n",
    "        self.ds = (self.data.train_ds if ds_type == DatasetType.Train else\n",
    "                   self.data.test_ds if ds_type == DatasetType.Test else\n",
    "                   self.data.valid_ds if ds_type == DatasetType.Valid else\n",
    "                   self.data.single_ds if ds_type == DatasetType.Single else\n",
    "                   self.data.fix_ds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_learner(cls, learn: Learner,  ds_type:DatasetType=DatasetType.Valid):\n",
    "        \"Gets preds, y_true, losses to construct base class from a learner\"\n",
    "        preds_res = learn.get_preds(ds_type=ds_type, with_loss=True)\n",
    "        return cls(learn, *preds_res)\n",
    "\n",
    "    def top_losses(self, k:int=None, largest=True):\n",
    "        \"`k` largest(/smallest) losses and indexes, defaulting to all losses (sorted by `largest`).\"\n",
    "        return self.losses.topk(ifnone(k, len(self.losses)), largest=largest)\n",
    "\n",
    "    # def top_scores(self, metric:Callable=None, k:int=None, largest=True):\n",
    "    #     \"`k` largest(/smallest) metric scores and indexes, defaulting to all scores (sorted by `largest`).\"\n",
    "    #     self.scores = metric(self.preds, self.y_true) \n",
    "    #     return self.scores.topk(ifnone(k, len(self.scores)), largest=largest)\n",
    "\n",
    "\n",
    "class ClassificationInterpretation(Interpretation):\n",
    "    \"Interpretation methods for classification models.\"\n",
    "    def __init__(self, learn:Learner, preds:Tensor, y_true:Tensor, losses:Tensor, ds_type:DatasetType=DatasetType.Valid):\n",
    "        super(ClassificationInterpretation, self).__init__(learn,preds,y_true,losses,ds_type)\n",
    "        self.pred_class = self.preds.argmax(dim=1)\n",
    "\n",
    "    def confusion_matrix(self, slice_size:int=1):\n",
    "        \"Confusion matrix as an `np.ndarray`.\"\n",
    "        x=torch.arange(0,self.data.c)\n",
    "        if slice_size is None: cm = ((self.pred_class==x[:,None]) & (self.y_true==x[:,None,None])).sum(2)\n",
    "        else:\n",
    "            cm = torch.zeros(self.data.c, self.data.c, dtype=x.dtype)\n",
    "            for i in range(0, self.y_true.shape[0], slice_size):\n",
    "                cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None])\n",
    "                            & (self.y_true[i:i+slice_size]==x[:,None,None])).sum(2)\n",
    "                torch.add(cm, cm_slice, out=cm)\n",
    "        return to_np(cm)\n",
    "\n",
    "    def plot_confusion_matrix(self, normalize:bool=False, title:str='Confusion matrix', cmap:Any=\"Blues\", slice_size:int=1,\n",
    "                              norm_dec:int=2, plot_txt:bool=True, return_fig:bool=None, **kwargs)->Optional[plt.Figure]:\n",
    "        \"Plot the confusion matrix, with `title` and using `cmap`.\"\n",
    "        # This function is mainly copied from the sklearn docs\n",
    "        cm = self.confusion_matrix(slice_size=slice_size)\n",
    "        if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fig = plt.figure(**kwargs)\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        tick_marks = np.arange(self.data.c)\n",
    "        plt.xticks(tick_marks, self.data.y.classes, rotation=90)\n",
    "        plt.yticks(tick_marks, self.data.y.classes, rotation=0)\n",
    "\n",
    "        if plot_txt:\n",
    "            thresh = cm.max() / 2.\n",
    "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                coeff = f'{cm[i, j]:.{norm_dec}f}' if normalize else f'{cm[i, j]}'\n",
    "                plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.grid(False)\n",
    "        if ifnone(return_fig, defaults.return_fig): return fig\n",
    "\n",
    "    def most_confused(self, min_val:int=1, slice_size:int=1)->Collection[Tuple[str,str,int]]:\n",
    "        \"Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences.\"\n",
    "        cm = self.confusion_matrix(slice_size=slice_size)\n",
    "        np.fill_diagonal(cm, 0)\n",
    "        res = [(self.data.classes[i],self.data.classes[j],cm[i,j])\n",
    "                for i,j in zip(*np.where(cm>=min_val))]\n",
    "        return sorted(res, key=itemgetter(2), reverse=True)\n",
    "\n",
    "\n",
    "def _learner_interpret(learn:Learner, ds_type:DatasetType=DatasetType.Valid):\n",
    "    \"Create a `ClassificationInterpretation` object from `learner` on `ds_type` with `tta`.\"\n",
    "    return ClassificationInterpretation.from_learner(learn, ds_type=ds_type)\n",
    "Learner.interpret = _learner_interpret\n",
    "\n",
    "class MultiLabelClassificationInterpretation(Interpretation):\n",
    "    \"Interpretation methods for classification models.\"\n",
    "    def __init__(self, learn:Learner, preds:Tensor, y_true:Tensor, losses:Tensor, ds_type:DatasetType=DatasetType.Valid,\n",
    "                     sigmoid:bool=True, thresh:float=0.3):\n",
    "        raise NotImplementedError\n",
    "        super(MultiLabelClassificationInterpretation, self).__init__(learn,preds,y_true,losses,ds_type)\n",
    "        self.pred_class = self.preds.sigmoid(dim=1)>thresh if sigmoid else self.preds>thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
