{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"`Learner` support for computer vision\"\n",
    "from torch_core import *\n",
    "from basic_train import *\n",
    "from basic_data import *\n",
    "from image import *\n",
    "#from . import models\n",
    "from callback import *\n",
    "from layers import *\n",
    "from hooks import *\n",
    "from train import ClassificationInterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "?bn_drop_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "?is_pool_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "?AdaptiveConcatPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__all__ = ['cnn_learner', 'create_cnn', 'create_cnn_model', 'create_body', 'create_head', 'unet_learner']\n",
    "# By default split models between first and second layer\n",
    "def _default_split(m:nn.Module): return (m[1],)\n",
    "# Split a resnet style model\n",
    "def _resnet_split(m:nn.Module): return (m[0][6],m[1])\n",
    "# Split squeezenet model on maxpool layers\n",
    "def _squeezenet_split(m:nn.Module): return (m[0][0][5], m[0][0][8], m[1])\n",
    "def _densenet_split(m:nn.Module): return (m[0][0][7],m[1])\n",
    "def _vgg_split(m:nn.Module): return (m[0][0][22],m[1])\n",
    "def _alexnet_split(m:nn.Module): return (m[0][0][6],m[1])\n",
    "\n",
    "_default_meta    = {'cut':None, 'split':_default_split}\n",
    "_resnet_meta     = {'cut':-2, 'split':_resnet_split }\n",
    "_squeezenet_meta = {'cut':-1, 'split': _squeezenet_split}\n",
    "_densenet_meta   = {'cut':-1, 'split':_densenet_split}\n",
    "_vgg_meta        = {'cut':-1, 'split':_vgg_split}\n",
    "_alexnet_meta    = {'cut':-1, 'split':_alexnet_split}\n",
    "\n",
    "model_meta = {\n",
    "    models.resnet18 :{**_resnet_meta}, models.resnet34: {**_resnet_meta},\n",
    "    models.resnet50 :{**_resnet_meta}, models.resnet101:{**_resnet_meta},\n",
    "    models.resnet152:{**_resnet_meta},\n",
    "\n",
    "    models.squeezenet1_0:{**_squeezenet_meta},\n",
    "    models.squeezenet1_1:{**_squeezenet_meta},\n",
    "\n",
    "    models.densenet121:{**_densenet_meta}, models.densenet169:{**_densenet_meta},\n",
    "    models.densenet201:{**_densenet_meta}, models.densenet161:{**_densenet_meta},\n",
    "    models.vgg16_bn:{**_vgg_meta}, models.vgg19_bn:{**_vgg_meta},\n",
    "    models.alexnet:{**_alexnet_meta}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__._resnet_split(m:torch.nn.modules.module.Module)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_meta.get(models.resnet18).get('split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataBunch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-74d35185ba58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mlin_ftrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_head\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0msplit_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSplitFuncOrIdxList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_final\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 concat_pool:bool=True, **kwargs:Any)->Learner:\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;34m\"Build convnet style learner.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataBunch' is not defined"
     ]
    }
   ],
   "source": [
    "def cnn_config(arch):\n",
    "    \"Get the metadata associated with `arch`.\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    return model_meta.get(arch, _default_meta)\n",
    "\n",
    "def has_pool_type(m):\n",
    "    if is_pool_type(m): return True\n",
    "    for l in m.children():\n",
    "        if has_pool_type(l): return True\n",
    "    return False\n",
    "\n",
    "def create_body(arch:Callable, pretrained:bool=True, cut:Optional[Union[int, Callable]]=None):\n",
    "    \"Cut off the body of a typically pretrained `model` at `cut` (int) or cut the model as specified by `cut(model)` (function).\"\n",
    "    model = arch(pretrained)\n",
    "    cut = ifnone(cut, cnn_config(arch)['cut'])\n",
    "    if cut is None:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "    if   isinstance(cut, int):      return nn.Sequential(*list(model.children())[:cut])\n",
    "    elif isinstance(cut, Callable): return cut(model)\n",
    "    else:                           raise NamedError(\"cut must be either integer or a function\")\n",
    "\n",
    "\n",
    "def create_head(nf:int, nc:int, lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5,\n",
    "                concat_pool:bool=True, bn_final:bool=False):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.\"\n",
    "    lin_ftrs = [nf, 512, nc] if lin_ftrs is None else [nf] + lin_ftrs + [nc]\n",
    "    ps = listify(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    layers = [pool, Flatten()]\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += bn_drop_lin(ni, no, True, p, actn)\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def create_cnn_model(base_arch:Callable, nc:int, cut:Union[int,Callable]=None, pretrained:bool=True,\n",
    "        lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5, custom_head:Optional[nn.Module]=None,\n",
    "        split_on:Optional[SplitFuncOrIdxList]=None, bn_final:bool=False, concat_pool:bool=True):\n",
    "    \"Create custom convnet architecture\"\n",
    "    body = create_body(base_arch, pretrained, cut)\n",
    "    if custom_head is None:\n",
    "        nf = num_features_model(nn.Sequential(*body.children())) * (2 if concat_pool else 1)\n",
    "        head = create_head(nf, nc, lin_ftrs, ps=ps, concat_pool=concat_pool, bn_final=bn_final)\n",
    "    else: head = custom_head\n",
    "    return nn.Sequential(body, head)\n",
    "\n",
    "def cnn_learner(data:DataBunch, base_arch:Callable, cut:Union[int,Callable]=None, pretrained:bool=True,\n",
    "                lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5, custom_head:Optional[nn.Module]=None,\n",
    "                split_on:Optional[SplitFuncOrIdxList]=None, bn_final:bool=False, init=nn.init.kaiming_normal_,\n",
    "                concat_pool:bool=True, **kwargs:Any)->Learner:\n",
    "    \"Build convnet style learner.\"\n",
    "    meta = cnn_config(base_arch)\n",
    "    model = create_cnn_model(base_arch, data.c, cut, pretrained, lin_ftrs, ps=ps, custom_head=custom_head,\n",
    "        split_on=split_on, bn_final=bn_final, concat_pool=concat_pool)\n",
    "    learn = Learner(data, model, **kwargs)\n",
    "    learn.split(split_on or meta['split'])\n",
    "    if pretrained: learn.freeze()\n",
    "    if init: apply_init(model[1], init)\n",
    "    return learn\n",
    "\n",
    "def create_cnn(data, base_arch, **kwargs):\n",
    "    warn(\"`create_cnn` is deprecated and is now named `cnn_learner`.\")\n",
    "    return cnn_learner(data, base_arch, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n",
    "                 norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, blur:bool=False,\n",
    "                 self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, last_cross:bool=True,\n",
    "                 bottle:bool=False, cut:Union[int,Callable]=None, **learn_kwargs:Any)->Learner:\n",
    "    \"Build Unet learner from `data` and `arch`.\"\n",
    "    meta = cnn_config(arch)\n",
    "    body = create_body(arch, pretrained, cut)\n",
    "    model = to_device(models.unet.DynamicUnet(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n",
    "          self_attention=self_attention, y_range=y_range, norm_type=norm_type, last_cross=last_cross,\n",
    "          bottle=bottle), data.device)\n",
    "    learn = Learner(data, model, **learn_kwargs)\n",
    "    learn.split(ifnone(split_on, meta['split']))\n",
    "    if pretrained: learn.freeze()\n",
    "    apply_init(model[2], nn.init.kaiming_normal_)\n",
    "    return learn\n",
    "\n",
    "@classmethod\n",
    "def _cl_int_from_learner(cls, learn:Learner, ds_type:DatasetType=DatasetType.Valid, tta=False):\n",
    "    \"Create an instance of `ClassificationInterpretation`. `tta` indicates if we want to use Test Time Augmentation.\"\n",
    "    preds = learn.TTA(ds_type=ds_type, with_loss=True) if tta else learn.get_preds(ds_type=ds_type, with_loss=True)\n",
    "    return cls(learn, *preds, ds_type=ds_type)\n",
    "\n",
    "def _test_cnn(m):\n",
    "    if not isinstance(m, nn.Sequential) or not len(m) == 2: return False\n",
    "    return isinstance(m[1][0], (AdaptiveConcatPool2d, nn.AdaptiveAvgPool2d))\n",
    "\n",
    "def _cl_int_plot_top_losses(self, k, largest=True, figsize=(12,12), heatmap:bool=None, heatmap_thresh:int=16,\n",
    "                            return_fig:bool=None)->Optional[plt.Figure]:\n",
    "    \"Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.\"\n",
    "    assert not heatmap or _test_cnn(self.learn.model), \"`heatmap=True` requires a model like `cnn_learner` produces.\"\n",
    "    if heatmap is None: heatmap = _test_cnn(self.learn.model)\n",
    "    tl_val,tl_idx = self.top_losses(k, largest)\n",
    "    classes = self.data.classes\n",
    "    cols = math.ceil(math.sqrt(k))\n",
    "    rows = math.ceil(k/cols)\n",
    "    fig,axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    fig.suptitle('prediction/actual/loss/probability', weight='bold', size=14)\n",
    "    for i,idx in enumerate(tl_idx):\n",
    "        im,cl = self.data.dl(self.ds_type).dataset[idx]\n",
    "        cl = int(cl)\n",
    "        im.show(ax=axes.flat[i], title=\n",
    "            f'{classes[self.pred_class[idx]]}/{classes[cl]} / {self.losses[idx]:.2f} / {self.preds[idx][cl]:.2f}')\n",
    "        if heatmap:\n",
    "            xb,_ = self.data.one_item(im, detach=False, denorm=False)\n",
    "            m = self.learn.model.eval()\n",
    "            with hook_output(m[0]) as hook_a:\n",
    "                with hook_output(m[0], grad= True) as hook_g:\n",
    "                    preds = m(xb)\n",
    "                    preds[0,cl].backward()\n",
    "            acts = hook_a.stored[0].cpu()\n",
    "            if (acts.shape[-1]*acts.shape[-2]) >= heatmap_thresh:\n",
    "                grad = hook_g.stored[0][0].cpu()\n",
    "                grad_chan = grad.mean(1).mean(1)\n",
    "                mult = F.relu(((acts*grad_chan[...,None,None])).sum(0))\n",
    "                sz = list(im.shape[-2:])\n",
    "                axes.flat[i].imshow(mult, alpha=0.6, extent=(0,*sz[::-1],0), interpolation='bilinear', cmap='magma')                \n",
    "    if ifnone(return_fig, defaults.return_fig): return fig\n",
    "\n",
    "def _cl_int_plot_multi_top_losses(self, samples:int=3, figsize:Tuple[int,int]=(8,8), save_misclassified:bool=False):\n",
    "    \"Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class in a multilabeled dataset.\"\n",
    "    if samples >20:\n",
    "        print(\"Max 20 samples\")\n",
    "        return\n",
    "    losses, idxs = self.top_losses(self.data.c)\n",
    "    l_dim = len(losses.size())\n",
    "    if l_dim == 1: losses, idxs = self.top_losses()\n",
    "    infolist, ordlosses_idxs, mismatches_idxs, mismatches, losses_mismatches, mismatchescontainer = [],[],[],[],[],[]\n",
    "    truthlabels = np.asarray(self.y_true, dtype=int)\n",
    "    classes_ids = [k for k in enumerate(self.data.classes)]\n",
    "    predclass = np.asarray(self.pred_class)\n",
    "    for i,pred in enumerate(predclass):\n",
    "        where_truth = np.nonzero((truthlabels[i]>0))[0]\n",
    "        mismatch = np.all(pred!=where_truth)\n",
    "        if mismatch:\n",
    "            mismatches_idxs.append(i)\n",
    "            if l_dim > 1 : losses_mismatches.append((losses[i][pred], i))\n",
    "            else: losses_mismatches.append((losses[i], i))\n",
    "        if l_dim > 1: infotup = (i, pred, where_truth, losses[i][pred], np.round(self.preds[i], decimals=3)[pred], mismatch)\n",
    "        else: infotup = (i, pred, where_truth, losses[i], np.round(self.preds[i], decimals=3)[pred], mismatch)\n",
    "        infolist.append(infotup)\n",
    "    ds = self.data.dl(self.ds_type).dataset\n",
    "    mismatches = ds[mismatches_idxs]\n",
    "    ordlosses = sorted(losses_mismatches, key = lambda x: x[0], reverse=True)\n",
    "    for w in ordlosses: ordlosses_idxs.append(w[1])\n",
    "    mismatches_ordered_byloss = ds[ordlosses_idxs]\n",
    "    print(f'{str(len(mismatches))} misclassified samples over {str(len(self.data.valid_ds))} samples in the validation set.')\n",
    "    samples = min(samples, len(mismatches))\n",
    "    for ima in range(len(mismatches_ordered_byloss)):\n",
    "        mismatchescontainer.append(mismatches_ordered_byloss[ima][0])\n",
    "    for sampleN in range(samples):\n",
    "        actualclasses = ''\n",
    "        for clas in infolist[ordlosses_idxs[sampleN]][2]:\n",
    "            actualclasses = f'{actualclasses} -- {str(classes_ids[clas][1])}'\n",
    "        imag = mismatches_ordered_byloss[sampleN][0]\n",
    "        imag = show_image(imag, figsize=figsize)\n",
    "        imag.set_title(f\"\"\"Predicted: {classes_ids[infolist[ordlosses_idxs[sampleN]][1]][1]} \\nActual: {actualclasses}\\nLoss: {infolist[ordlosses_idxs[sampleN]][3]}\\nProbability: {infolist[ordlosses_idxs[sampleN]][4]}\"\"\",\n",
    "                        loc='left')\n",
    "        plt.show()\n",
    "        if save_misclassified: return mismatchescontainer\n",
    "\n",
    "ClassificationInterpretation.from_learner          = _cl_int_from_learner\n",
    "ClassificationInterpretation.plot_top_losses       = _cl_int_plot_top_losses\n",
    "ClassificationInterpretation.plot_multi_top_losses = _cl_int_plot_multi_top_losses\n",
    "\n",
    "def _learner_interpret(learn:Learner, ds_type:DatasetType=DatasetType.Valid, tta=False):\n",
    "    \"Create a `ClassificationInterpretation` object from `learner` on `ds_type` with `tta`.\"\n",
    "    return ClassificationInterpretation.from_learner(learn, ds_type=ds_type, tta=tta)\n",
    "Learner.interpret = _learner_interpret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
