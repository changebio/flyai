{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from flyai.dataset import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from model import Model\n",
    "from path import MODEL_PATH\n",
    "\n",
    "\n",
    "\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, x_test, y_test):\n",
    "    cnn.eval()\n",
    "    batch_eval = model.batch_iter(x_test, y_test)\n",
    "    total_acc = 0.0\n",
    "    data_len = len(x_test)\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        outputs = cnn(x_batch)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        correct = (prediction == y_batch).sum().item()\n",
    "        acc = correct / batch_len\n",
    "        total_acc += acc * batch_len\n",
    "    return total_acc / data_len\n",
    "\n",
    "outputs = cnn(x_train)\n",
    "_, prediction = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(p,y):\n",
    "    _,yp = torch.max(p.data,1)\n",
    "    return (yp == y).sum().item()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(data,net,loss,ep=10,bz=64,gpu=True):\n",
    "    net.train()\n",
    "    ave_loss=0\n",
    "    ave_score=0\n",
    "    for _ in range(ep):\n",
    "        #read data batch by batch\n",
    "        x_train, y_train, _ , _ = data.next_batch(bz) \n",
    "        x_train = torch.from_numpy(x_train)\n",
    "        y_train = torch.from_numpy(y_train)\n",
    "        x_train = x_train.float()\n",
    "\n",
    "        if gpu:\n",
    "            x_train = Variable(x_train.cuda())\n",
    "            y_train = Variable(y_train.cuda())\n",
    "        \n",
    "        y_outs = net(x_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(net(x_train), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ave_loss += loss.data\n",
    "        ave_score +=score(y_outs,y_train)\n",
    "        print(ave_loss/ep,ave_score/ep)\n",
    "    return net\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4654, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(0.9304, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(1.3952, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(1.8597, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(2.3239, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(2.7879, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(3.2516, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(3.7151, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(4.1784, device='cuda:0') tensor(0, device='cuda:0')\n",
      "tensor(4.6414, device='cuda:0') tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "net = train_net(data,cnn,1,ep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0036, 0.0093, 0.0116,  ..., 0.0078, 0.0075, 0.6458],\n",
       "        [0.0319, 0.0343, 0.0309,  ..., 0.0247, 0.0300, 0.5325],\n",
       "        [0.0104, 0.0093, 0.0159,  ..., 0.0082, 0.0091, 0.7882],\n",
       "        ...,\n",
       "        [0.0129, 0.0181, 0.0278,  ..., 0.0062, 0.0112, 0.5283],\n",
       "        [0.0380, 0.0279, 0.0114,  ..., 0.0059, 0.0056, 0.6189],\n",
       "        [0.0077, 0.0065, 0.0136,  ..., 0.0054, 0.0034, 0.8602]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-debf4cef1c62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义DenseNet实例,加载与训练模型，并更改最后一层\n",
    "cnn = torchvision.models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changebio/anaconda3/envs/fastv1/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "cnn1 = torchvision.models.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fc.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = torchvision.models.resnet34(pretrained=True)\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "num_features = cnn.fc.in_features\n",
    "cnn.fc = nn.Sequential(nn.Linear(num_features, 256),nn.Sigmoid())\n",
    "\n",
    "if cuda_avail:\n",
    "    cnn.cuda()\n",
    "\n",
    "optimizer = Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.999))  # 选用AdamOptimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # 定义损失函数\n",
    "\n",
    "# 训练并评估模型\n",
    "\n",
    "data = Dataset()\n",
    "model = Model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, x_test, y_test):\n",
    "    cnn.eval()\n",
    "    batch_eval = model.batch_iter(x_test, y_test)\n",
    "    total_acc = 0.0\n",
    "    data_len = len(x_test)\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        outputs = cnn(x_batch)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        correct = (prediction == y_batch).sum().item()\n",
    "        acc = correct / batch_len\n",
    "        total_acc += acc * batch_len\n",
    "    return total_acc / data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.train()\n",
    "x_train, y_train, x_test, y_test = data.next_batch(32)  # 读取数据\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_train = x_train.float()\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "x_test = x_test.float()\n",
    "\n",
    "if cuda_avail:\n",
    "    x_train = Variable(x_train.cuda())\n",
    "    y_train = Variable(y_train.cuda())\n",
    "    x_test = Variable(x_test.cuda())\n",
    "    y_test = Variable(y_test.cuda())\n",
    "\n",
    "outputs = cnn(x_train)\n",
    "_, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss = loss_fn(outputs, y_train)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5358, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, best accuracy 0.03125\n",
      "0/100\n",
      "1/100\n",
      "2/100\n",
      "3/100\n",
      "step 4, best accuracy 0.0625\n",
      "4/100\n",
      "5/100\n",
      "6/100\n",
      "7/100\n",
      "8/100\n",
      "9/100\n",
      "10/100\n",
      "11/100\n",
      "12/100\n",
      "13/100\n",
      "14/100\n",
      "15/100\n",
      "16/100\n",
      "17/100\n",
      "step 18, best accuracy 0.09375\n",
      "18/100\n",
      "19/100\n",
      "20/100\n",
      "21/100\n",
      "22/100\n",
      "23/100\n",
      "24/100\n",
      "25/100\n",
      "26/100\n",
      "27/100\n",
      "28/100\n",
      "29/100\n",
      "30/100\n",
      "31/100\n",
      "32/100\n",
      "33/100\n",
      "34/100\n",
      "35/100\n",
      "36/100\n",
      "37/100\n",
      "38/100\n",
      "39/100\n",
      "40/100\n",
      "41/100\n",
      "42/100\n",
      "43/100\n",
      "44/100\n",
      "45/100\n",
      "46/100\n",
      "47/100\n",
      "48/100\n",
      "49/100\n",
      "50/100\n",
      "51/100\n",
      "52/100\n",
      "53/100\n",
      "54/100\n",
      "55/100\n",
      "56/100\n",
      "57/100\n",
      "58/100\n",
      "59/100\n",
      "60/100\n",
      "61/100\n",
      "62/100\n",
      "63/100\n",
      "64/100\n",
      "65/100\n",
      "66/100\n",
      "67/100\n",
      "68/100\n",
      "69/100\n",
      "70/100\n",
      "71/100\n",
      "72/100\n",
      "73/100\n",
      "74/100\n",
      "75/100\n",
      "76/100\n",
      "77/100\n",
      "78/100\n",
      "79/100\n",
      "80/100\n",
      "81/100\n",
      "82/100\n",
      "83/100\n",
      "84/100\n",
      "85/100\n",
      "86/100\n",
      "87/100\n",
      "88/100\n",
      "89/100\n",
      "90/100\n",
      "91/100\n",
      "92/100\n",
      "93/100\n",
      "94/100\n",
      "95/100\n",
      "96/100\n",
      "97/100\n",
      "98/100\n",
      "99/100\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for i in range(100):\n",
    "    cnn.train()\n",
    "    x_train, y_train, x_test, y_test = data.next_batch(32)  # 读取数据\n",
    "\n",
    "    x_train = torch.from_numpy(x_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    x_train = x_train.float()\n",
    "\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    x_test = x_test.float()\n",
    "\n",
    "    if cuda_avail:\n",
    "        x_train = Variable(x_train.cuda())\n",
    "        y_train = Variable(y_train.cuda())\n",
    "        x_test = Variable(x_test.cuda())\n",
    "        y_test = Variable(y_test.cuda())\n",
    "\n",
    "    outputs = cnn(x_train)\n",
    "    _, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 若测试准确率高于当前最高准确率，则保存模型\n",
    "    train_accuracy = eval(model, x_test, y_test)\n",
    "    if train_accuracy > best_accuracy:\n",
    "        best_accuracy = train_accuracy\n",
    "        model.save_model(cnn, MODEL_PATH, overwrite=True)\n",
    "        print(\"step %d, best accuracy %g\" % (i, best_accuracy))\n",
    "\n",
    "    print(str(i) + \"/\" + str(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'image_path': 'image/097.harmonica/097_0010.jpg'},\n",
       "  {'image_path': 'image/145.motorbikes-101/145_0455.jpg'},\n",
       "  {'image_path': 'image/057.dolphin-101/057_0072.jpg'},\n",
       "  {'image_path': 'image/192.snowmobile/192_0093.jpg'},\n",
       "  {'image_path': 'image/154.palm-tree/154_0101.jpg'},\n",
       "  {'image_path': 'image/073.fireworks/073_0092.jpg'},\n",
       "  {'image_path': 'image/070.fire-extinguisher/070_0023.jpg'},\n",
       "  {'image_path': 'image/256.toad/256_0051.jpg'},\n",
       "  {'image_path': 'image/234.tweezer/234_0020.jpg'},\n",
       "  {'image_path': 'image/230.trilobite-101/230_0005.jpg'},\n",
       "  {'image_path': 'image/126.ladder/126_0201.jpg'},\n",
       "  {'image_path': 'image/177.saturn/177_0059.jpg'},\n",
       "  {'image_path': 'image/218.tennis-racket/218_0068.jpg'},\n",
       "  {'image_path': 'image/105.horse/105_0036.jpg'},\n",
       "  {'image_path': 'image/106.horseshoe-crab/106_0006.jpg'},\n",
       "  {'image_path': 'image/164.porcupine/164_0096.jpg'},\n",
       "  {'image_path': 'image/240.watch-101/240_0038.jpg'},\n",
       "  {'image_path': 'image/039.chopsticks/039_0038.jpg'},\n",
       "  {'image_path': 'image/005.baseball-glove/005_0136.jpg'},\n",
       "  {'image_path': 'image/051.cowboy-hat/051_0089.jpg'},\n",
       "  {'image_path': 'image/096.hammock/096_0275.jpg'},\n",
       "  {'image_path': 'image/235.umbrella-101/235_0078.jpg'},\n",
       "  {'image_path': 'image/033.cd/033_0087.jpg'},\n",
       "  {'image_path': 'image/175.roulette-wheel/175_0058.jpg'},\n",
       "  {'image_path': 'image/145.motorbikes-101/145_0624.jpg'},\n",
       "  {'image_path': 'image/132.light-house/132_0094.jpg'},\n",
       "  {'image_path': 'image/092.grapes/092_0051.jpg'},\n",
       "  {'image_path': 'image/212.teapot/212_0112.jpg'},\n",
       "  {'image_path': 'image/014.blimp/014_0036.jpg'},\n",
       "  {'image_path': 'image/214.teepee/214_0079.jpg'},\n",
       "  {'image_path': 'image/232.t-shirt/232_0090.jpg'},\n",
       "  {'image_path': 'image/170.rainbow/170_0084.jpg'},\n",
       "  {'image_path': 'image/011.billiards/011_0166.jpg'},\n",
       "  {'image_path': 'image/230.trilobite-101/230_0002.jpg'},\n",
       "  {'image_path': 'image/055.dice/055_0024.jpg'},\n",
       "  {'image_path': 'image/045.computer-keyboard/045_0071.jpg'},\n",
       "  {'image_path': 'image/124.killer-whale/124_0057.jpg'},\n",
       "  {'image_path': 'image/222.tombstone/222_0084.jpg'},\n",
       "  {'image_path': 'image/019.boxing-glove/019_0057.jpg'},\n",
       "  {'image_path': 'image/241.waterfall/241_0010.jpg'},\n",
       "  {'image_path': 'image/096.hammock/096_0210.jpg'},\n",
       "  {'image_path': 'image/144.minotaur/144_0060.jpg'},\n",
       "  {'image_path': 'image/059.drinking-straw/059_0027.jpg'},\n",
       "  {'image_path': 'image/087.goldfish/087_0090.jpg'},\n",
       "  {'image_path': 'image/028.camel/028_0050.jpg'},\n",
       "  {'image_path': 'image/250.zebra/250_0016.jpg'},\n",
       "  {'image_path': 'image/035.cereal-box/035_0084.jpg'},\n",
       "  {'image_path': 'image/108.hot-dog/108_0005.jpg'},\n",
       "  {'image_path': 'image/029.cannon/029_0002.jpg'},\n",
       "  {'image_path': 'image/014.blimp/014_0077.jpg'},\n",
       "  {'image_path': 'image/092.grapes/092_0101.jpg'},\n",
       "  {'image_path': 'image/090.gorilla/090_0098.jpg'},\n",
       "  {'image_path': 'image/019.boxing-glove/019_0076.jpg'},\n",
       "  {'image_path': 'image/122.kayak/122_0069.jpg'},\n",
       "  {'image_path': 'image/192.snowmobile/192_0026.jpg'},\n",
       "  {'image_path': 'image/124.killer-whale/124_0074.jpg'},\n",
       "  {'image_path': 'image/145.motorbikes-101/145_0324.jpg'},\n",
       "  {'image_path': 'image/166.praying-mantis/166_0081.jpg'},\n",
       "  {'image_path': 'image/140.menorah-101/140_0069.jpg'},\n",
       "  {'image_path': 'image/163.playing-card/163_0005.jpg'},\n",
       "  {'image_path': 'image/123.ketch-101/123_0076.jpg'},\n",
       "  {'image_path': 'image/050.covered-wagon/050_0012.jpg'},\n",
       "  {'image_path': 'image/082.galaxy/082_0066.jpg'},\n",
       "  {'image_path': 'image/232.t-shirt/232_0254.jpg'},\n",
       "  {'image_path': 'image/181.segway/181_0016.jpg'},\n",
       "  {'image_path': 'image/149.necktie/149_0003.jpg'},\n",
       "  {'image_path': 'image/244.wheelbarrow/244_0025.jpg'},\n",
       "  {'image_path': 'image/092.grapes/092_0133.jpg'},\n",
       "  {'image_path': 'image/071.fire-hydrant/071_0009.jpg'},\n",
       "  {'image_path': 'image/082.galaxy/082_0037.jpg'},\n",
       "  {'image_path': 'image/236.unicorn/236_0043.jpg'},\n",
       "  {'image_path': 'image/232.t-shirt/232_0094.jpg'},\n",
       "  {'image_path': 'image/217.tennis-court/217_0036.jpg'},\n",
       "  {'image_path': 'image/124.killer-whale/124_0073.jpg'},\n",
       "  {'image_path': 'image/133.lightning/133_0030.jpg'},\n",
       "  {'image_path': 'image/011.billiards/011_0259.jpg'},\n",
       "  {'image_path': 'image/209.sword/209_0054.jpg'},\n",
       "  {'image_path': 'image/184.sheet-music/184_0003.jpg'},\n",
       "  {'image_path': 'image/156.paper-shredder/156_0064.jpg'},\n",
       "  {'image_path': 'image/185.skateboard/185_0017.jpg'},\n",
       "  {'image_path': 'image/150.octopus/150_0054.jpg'},\n",
       "  {'image_path': 'image/213.teddy-bear/213_0031.jpg'},\n",
       "  {'image_path': 'image/096.hammock/096_0220.jpg'},\n",
       "  {'image_path': 'image/098.harp/098_0017.jpg'},\n",
       "  {'image_path': 'image/018.bowling-pin/018_0050.jpg'},\n",
       "  {'image_path': 'image/128.lathe/128_0058.jpg'},\n",
       "  {'image_path': 'image/116.iguana/116_0014.jpg'},\n",
       "  {'image_path': 'image/085.goat/085_0027.jpg'},\n",
       "  {'image_path': 'image/080.frog/080_0013.jpg'},\n",
       "  {'image_path': 'image/117.ipod/117_0080.jpg'},\n",
       "  {'image_path': 'image/043.coin/043_0061.jpg'},\n",
       "  {'image_path': 'image/242.watermelon/242_0043.jpg'},\n",
       "  {'image_path': 'image/048.conch/048_0025.jpg'},\n",
       "  {'image_path': 'image/008.bathtub/008_0050.jpg'},\n",
       "  {'image_path': 'image/090.gorilla/090_0152.jpg'},\n",
       "  {'image_path': 'image/115.ice-cream-cone/115_0007.jpg'},\n",
       "  {'image_path': 'image/128.lathe/128_0070.jpg'},\n",
       "  {'image_path': 'image/223.top-hat/223_0021.jpg'},\n",
       "  {'image_path': 'image/105.horse/105_0198.jpg'},\n",
       "  {'image_path': 'image/218.tennis-racket/218_0072.jpg'}],\n",
       " [{'label': 96},\n",
       "  {'label': 144},\n",
       "  {'label': 56},\n",
       "  {'label': 191},\n",
       "  {'label': 153},\n",
       "  {'label': 72},\n",
       "  {'label': 69},\n",
       "  {'label': 255},\n",
       "  {'label': 233},\n",
       "  {'label': 229},\n",
       "  {'label': 125},\n",
       "  {'label': 176},\n",
       "  {'label': 217},\n",
       "  {'label': 104},\n",
       "  {'label': 105},\n",
       "  {'label': 163},\n",
       "  {'label': 239},\n",
       "  {'label': 38},\n",
       "  {'label': 4},\n",
       "  {'label': 50},\n",
       "  {'label': 95},\n",
       "  {'label': 234},\n",
       "  {'label': 32},\n",
       "  {'label': 174},\n",
       "  {'label': 144},\n",
       "  {'label': 131},\n",
       "  {'label': 91},\n",
       "  {'label': 211},\n",
       "  {'label': 13},\n",
       "  {'label': 213},\n",
       "  {'label': 231},\n",
       "  {'label': 169},\n",
       "  {'label': 10},\n",
       "  {'label': 229},\n",
       "  {'label': 54},\n",
       "  {'label': 44},\n",
       "  {'label': 123},\n",
       "  {'label': 221},\n",
       "  {'label': 18},\n",
       "  {'label': 240},\n",
       "  {'label': 95},\n",
       "  {'label': 143},\n",
       "  {'label': 58},\n",
       "  {'label': 86},\n",
       "  {'label': 27},\n",
       "  {'label': 249},\n",
       "  {'label': 34},\n",
       "  {'label': 107},\n",
       "  {'label': 28},\n",
       "  {'label': 13},\n",
       "  {'label': 91},\n",
       "  {'label': 89},\n",
       "  {'label': 18},\n",
       "  {'label': 121},\n",
       "  {'label': 191},\n",
       "  {'label': 123},\n",
       "  {'label': 144},\n",
       "  {'label': 165},\n",
       "  {'label': 139},\n",
       "  {'label': 162},\n",
       "  {'label': 122},\n",
       "  {'label': 49},\n",
       "  {'label': 81},\n",
       "  {'label': 231},\n",
       "  {'label': 180},\n",
       "  {'label': 148},\n",
       "  {'label': 243},\n",
       "  {'label': 91},\n",
       "  {'label': 70},\n",
       "  {'label': 81},\n",
       "  {'label': 235},\n",
       "  {'label': 231},\n",
       "  {'label': 216},\n",
       "  {'label': 123},\n",
       "  {'label': 132},\n",
       "  {'label': 10},\n",
       "  {'label': 208},\n",
       "  {'label': 183},\n",
       "  {'label': 155},\n",
       "  {'label': 184},\n",
       "  {'label': 149},\n",
       "  {'label': 212},\n",
       "  {'label': 95},\n",
       "  {'label': 97},\n",
       "  {'label': 17},\n",
       "  {'label': 127},\n",
       "  {'label': 115},\n",
       "  {'label': 84},\n",
       "  {'label': 79},\n",
       "  {'label': 116},\n",
       "  {'label': 42},\n",
       "  {'label': 241},\n",
       "  {'label': 47},\n",
       "  {'label': 7},\n",
       "  {'label': 89},\n",
       "  {'label': 114},\n",
       "  {'label': 127},\n",
       "  {'label': 222},\n",
       "  {'label': 104},\n",
       "  {'label': 217}],\n",
       " [{'image_path': 'image/097.harmonica/097_0010.jpg'},\n",
       "  {'image_path': 'image/145.motorbikes-101/145_0455.jpg'},\n",
       "  {'image_path': 'image/057.dolphin-101/057_0072.jpg'},\n",
       "  {'image_path': 'image/192.snowmobile/192_0093.jpg'},\n",
       "  {'image_path': 'image/154.palm-tree/154_0101.jpg'},\n",
       "  {'image_path': 'image/073.fireworks/073_0092.jpg'},\n",
       "  {'image_path': 'image/070.fire-extinguisher/070_0023.jpg'},\n",
       "  {'image_path': 'image/256.toad/256_0051.jpg'},\n",
       "  {'image_path': 'image/234.tweezer/234_0020.jpg'},\n",
       "  {'image_path': 'image/230.trilobite-101/230_0005.jpg'},\n",
       "  {'image_path': 'image/126.ladder/126_0201.jpg'},\n",
       "  {'image_path': 'image/177.saturn/177_0059.jpg'},\n",
       "  {'image_path': 'image/218.tennis-racket/218_0068.jpg'},\n",
       "  {'image_path': 'image/105.horse/105_0036.jpg'},\n",
       "  {'image_path': 'image/106.horseshoe-crab/106_0006.jpg'},\n",
       "  {'image_path': 'image/164.porcupine/164_0096.jpg'},\n",
       "  {'image_path': 'image/240.watch-101/240_0038.jpg'},\n",
       "  {'image_path': 'image/039.chopsticks/039_0038.jpg'},\n",
       "  {'image_path': 'image/005.baseball-glove/005_0136.jpg'},\n",
       "  {'image_path': 'image/051.cowboy-hat/051_0089.jpg'},\n",
       "  {'image_path': 'image/096.hammock/096_0275.jpg'},\n",
       "  {'image_path': 'image/235.umbrella-101/235_0078.jpg'},\n",
       "  {'image_path': 'image/033.cd/033_0087.jpg'},\n",
       "  {'image_path': 'image/175.roulette-wheel/175_0058.jpg'},\n",
       "  {'image_path': 'image/145.motorbikes-101/145_0624.jpg'},\n",
       "  {'image_path': 'image/132.light-house/132_0094.jpg'},\n",
       "  {'image_path': 'image/092.grapes/092_0051.jpg'},\n",
       "  {'image_path': 'image/212.teapot/212_0112.jpg'},\n",
       "  {'image_path': 'image/014.blimp/014_0036.jpg'},\n",
       "  {'image_path': 'image/214.teepee/214_0079.jpg'},\n",
       "  {'image_path': 'image/232.t-shirt/232_0090.jpg'},\n",
       "  {'image_path': 'image/170.rainbow/170_0084.jpg'},\n",
       "  {'image_path': 'image/011.billiards/011_0166.jpg'},\n",
       "  {'image_path': 'image/230.trilobite-101/230_0002.jpg'},\n",
       "  {'image_path': 'image/055.dice/055_0024.jpg'},\n",
       "  {'image_path': 'image/045.computer-keyboard/045_0071.jpg'},\n",
       "  {'image_path': 'image/124.killer-whale/124_0057.jpg'},\n",
       "  {'image_path': 'image/222.tombstone/222_0084.jpg'},\n",
       "  {'image_path': 'image/019.boxing-glove/019_0057.jpg'},\n",
       "  {'image_path': 'image/241.waterfall/241_0010.jpg'},\n",
       "  {'image_path': 'image/096.hammock/096_0210.jpg'},\n",
       "  {'image_path': 'image/144.minotaur/144_0060.jpg'},\n",
       "  {'image_path': 'image/059.drinking-straw/059_0027.jpg'},\n",
       "  {'image_path': 'image/087.goldfish/087_0090.jpg'},\n",
       "  {'image_path': 'image/028.camel/028_0050.jpg'},\n",
       "  {'image_path': 'image/250.zebra/250_0016.jpg'},\n",
       "  {'image_path': 'image/035.cereal-box/035_0084.jpg'},\n",
       "  {'image_path': 'image/108.hot-dog/108_0005.jpg'},\n",
       "  {'image_path': 'image/029.cannon/029_0002.jpg'},\n",
       "  {'image_path': 'image/014.blimp/014_0077.jpg'},\n",
       "  {'image_path': 'image/092.grapes/092_0101.jpg'},\n",
       "  {'image_path': 'image/090.gorilla/090_0098.jpg'},\n",
       "  {'image_path': 'image/019.boxing-glove/019_0076.jpg'},\n",
       "  {'image_path': 'image/122.kayak/122_0069.jpg'},\n",
       "  {'image_path': 'image/192.snowmobile/192_0026.jpg'},\n",
       "  {'image_path': 'image/124.killer-whale/124_0074.jpg'},\n",
       "  {'image_path': 'image/145.motorbikes-101/145_0324.jpg'},\n",
       "  {'image_path': 'image/166.praying-mantis/166_0081.jpg'},\n",
       "  {'image_path': 'image/140.menorah-101/140_0069.jpg'},\n",
       "  {'image_path': 'image/163.playing-card/163_0005.jpg'},\n",
       "  {'image_path': 'image/123.ketch-101/123_0076.jpg'},\n",
       "  {'image_path': 'image/050.covered-wagon/050_0012.jpg'},\n",
       "  {'image_path': 'image/082.galaxy/082_0066.jpg'},\n",
       "  {'image_path': 'image/232.t-shirt/232_0254.jpg'},\n",
       "  {'image_path': 'image/181.segway/181_0016.jpg'},\n",
       "  {'image_path': 'image/149.necktie/149_0003.jpg'},\n",
       "  {'image_path': 'image/244.wheelbarrow/244_0025.jpg'},\n",
       "  {'image_path': 'image/092.grapes/092_0133.jpg'},\n",
       "  {'image_path': 'image/071.fire-hydrant/071_0009.jpg'},\n",
       "  {'image_path': 'image/082.galaxy/082_0037.jpg'},\n",
       "  {'image_path': 'image/236.unicorn/236_0043.jpg'},\n",
       "  {'image_path': 'image/232.t-shirt/232_0094.jpg'},\n",
       "  {'image_path': 'image/217.tennis-court/217_0036.jpg'},\n",
       "  {'image_path': 'image/124.killer-whale/124_0073.jpg'},\n",
       "  {'image_path': 'image/133.lightning/133_0030.jpg'},\n",
       "  {'image_path': 'image/011.billiards/011_0259.jpg'},\n",
       "  {'image_path': 'image/209.sword/209_0054.jpg'},\n",
       "  {'image_path': 'image/184.sheet-music/184_0003.jpg'},\n",
       "  {'image_path': 'image/156.paper-shredder/156_0064.jpg'},\n",
       "  {'image_path': 'image/185.skateboard/185_0017.jpg'},\n",
       "  {'image_path': 'image/150.octopus/150_0054.jpg'},\n",
       "  {'image_path': 'image/213.teddy-bear/213_0031.jpg'},\n",
       "  {'image_path': 'image/096.hammock/096_0220.jpg'},\n",
       "  {'image_path': 'image/098.harp/098_0017.jpg'},\n",
       "  {'image_path': 'image/018.bowling-pin/018_0050.jpg'},\n",
       "  {'image_path': 'image/128.lathe/128_0058.jpg'},\n",
       "  {'image_path': 'image/116.iguana/116_0014.jpg'},\n",
       "  {'image_path': 'image/085.goat/085_0027.jpg'},\n",
       "  {'image_path': 'image/080.frog/080_0013.jpg'},\n",
       "  {'image_path': 'image/117.ipod/117_0080.jpg'},\n",
       "  {'image_path': 'image/043.coin/043_0061.jpg'},\n",
       "  {'image_path': 'image/242.watermelon/242_0043.jpg'},\n",
       "  {'image_path': 'image/048.conch/048_0025.jpg'},\n",
       "  {'image_path': 'image/008.bathtub/008_0050.jpg'},\n",
       "  {'image_path': 'image/090.gorilla/090_0152.jpg'},\n",
       "  {'image_path': 'image/115.ice-cream-cone/115_0007.jpg'},\n",
       "  {'image_path': 'image/128.lathe/128_0070.jpg'},\n",
       "  {'image_path': 'image/223.top-hat/223_0021.jpg'},\n",
       "  {'image_path': 'image/105.horse/105_0198.jpg'},\n",
       "  {'image_path': 'image/218.tennis-racket/218_0072.jpg'}],\n",
       " [{'label': 96},\n",
       "  {'label': 144},\n",
       "  {'label': 56},\n",
       "  {'label': 191},\n",
       "  {'label': 153},\n",
       "  {'label': 72},\n",
       "  {'label': 69},\n",
       "  {'label': 255},\n",
       "  {'label': 233},\n",
       "  {'label': 229},\n",
       "  {'label': 125},\n",
       "  {'label': 176},\n",
       "  {'label': 217},\n",
       "  {'label': 104},\n",
       "  {'label': 105},\n",
       "  {'label': 163},\n",
       "  {'label': 239},\n",
       "  {'label': 38},\n",
       "  {'label': 4},\n",
       "  {'label': 50},\n",
       "  {'label': 95},\n",
       "  {'label': 234},\n",
       "  {'label': 32},\n",
       "  {'label': 174},\n",
       "  {'label': 144},\n",
       "  {'label': 131},\n",
       "  {'label': 91},\n",
       "  {'label': 211},\n",
       "  {'label': 13},\n",
       "  {'label': 213},\n",
       "  {'label': 231},\n",
       "  {'label': 169},\n",
       "  {'label': 10},\n",
       "  {'label': 229},\n",
       "  {'label': 54},\n",
       "  {'label': 44},\n",
       "  {'label': 123},\n",
       "  {'label': 221},\n",
       "  {'label': 18},\n",
       "  {'label': 240},\n",
       "  {'label': 95},\n",
       "  {'label': 143},\n",
       "  {'label': 58},\n",
       "  {'label': 86},\n",
       "  {'label': 27},\n",
       "  {'label': 249},\n",
       "  {'label': 34},\n",
       "  {'label': 107},\n",
       "  {'label': 28},\n",
       "  {'label': 13},\n",
       "  {'label': 91},\n",
       "  {'label': 89},\n",
       "  {'label': 18},\n",
       "  {'label': 121},\n",
       "  {'label': 191},\n",
       "  {'label': 123},\n",
       "  {'label': 144},\n",
       "  {'label': 165},\n",
       "  {'label': 139},\n",
       "  {'label': 162},\n",
       "  {'label': 122},\n",
       "  {'label': 49},\n",
       "  {'label': 81},\n",
       "  {'label': 231},\n",
       "  {'label': 180},\n",
       "  {'label': 148},\n",
       "  {'label': 243},\n",
       "  {'label': 91},\n",
       "  {'label': 70},\n",
       "  {'label': 81},\n",
       "  {'label': 235},\n",
       "  {'label': 231},\n",
       "  {'label': 216},\n",
       "  {'label': 123},\n",
       "  {'label': 132},\n",
       "  {'label': 10},\n",
       "  {'label': 208},\n",
       "  {'label': 183},\n",
       "  {'label': 155},\n",
       "  {'label': 184},\n",
       "  {'label': 149},\n",
       "  {'label': 212},\n",
       "  {'label': 95},\n",
       "  {'label': 97},\n",
       "  {'label': 17},\n",
       "  {'label': 127},\n",
       "  {'label': 115},\n",
       "  {'label': 84},\n",
       "  {'label': 79},\n",
       "  {'label': 116},\n",
       "  {'label': 42},\n",
       "  {'label': 241},\n",
       "  {'label': 47},\n",
       "  {'label': 7},\n",
       "  {'label': 89},\n",
       "  {'label': 114},\n",
       "  {'label': 127},\n",
       "  {'label': 222},\n",
       "  {'label': 104},\n",
       "  {'label': 217}])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 100, 100, 100]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in data.get_all_data()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "input_x() missing 1 required positional argument: 'image_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2b373c6c4d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastv1/lib/python3.6/site-packages/flyai/dataset.py\u001b[0m in \u001b[0;36mpredict_data\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mprocessor_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastv1/lib/python3.6/site-packages/flyai/dataset.py\u001b[0m in \u001b[0;36mget_method_dict\u001b[0;34m(self, clz, method_name, **args)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_method_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_method_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: input_x() missing 1 required positional argument: 'image_path'"
     ]
    }
   ],
   "source": [
    "data.predict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.6837916 ,  1.735166  ,  1.5639184 , ...,  1.6837916 ,\n",
       "           1.7009164 ,  1.1871738 ],\n",
       "         [ 1.6666669 ,  1.6837916 ,  1.4954194 , ...,  1.5467936 ,\n",
       "           1.3412966 ,  0.6563062 ],\n",
       "         [ 1.3926709 ,  1.3584213 ,  1.1015497 , ...,  1.0673003 ,\n",
       "           0.8960528 ,  0.31381115],\n",
       "         ...,\n",
       "         [ 1.2556728 ,  1.0330508 ,  0.810429  , ...,  0.8446785 ,\n",
       "           0.5364329 ,  0.12543888],\n",
       "         [ 0.810429  ,  0.31381115,  0.05693987, ..., -0.01155914,\n",
       "          -0.0971829 , -0.1314324 ],\n",
       "         [ 0.17681314, -0.30267993, -0.21705617, ..., -0.25130567,\n",
       "          -0.16568191, -0.16568191]],\n",
       "\n",
       "        [[ 1.6232495 ,  1.6582636 ,  1.4306725 , ...,  1.4481796 ,\n",
       "           1.2556022 ,  0.69537824],\n",
       "         [ 1.4481796 ,  1.4481796 ,  1.1855743 , ...,  1.1680672 ,\n",
       "           0.74789923, -0.03991582],\n",
       "         [ 0.94047624,  0.87044823,  0.55532223, ...,  0.4677872 ,\n",
       "           0.04761919, -0.617647  ],\n",
       "         ...,\n",
       "         [ 0.55532223,  0.3102242 , -0.00490182, ..., -0.03991582,\n",
       "          -0.495098  , -0.98529404],\n",
       "         [-0.02240882, -0.582633  , -0.897759  , ..., -1.020308  ,\n",
       "          -1.177871  , -1.265406  ],\n",
       "         [-0.722689  , -1.247899  , -1.247899  , ..., -1.3354341 ,\n",
       "          -1.30042   , -1.317927  ]],\n",
       "\n",
       "        [[ 1.2282355 ,  1.193377  ,  0.8796516 , ...,  0.8970808 ,\n",
       "           0.287059  , -0.44496724],\n",
       "         [ 0.8447932 ,  0.7925056 ,  0.47878015, ...,  0.3916342 ,\n",
       "          -0.23581691, -1.0724182 ],\n",
       "         [ 0.04305032, -0.09638323, -0.44496724, ..., -0.68897593,\n",
       "          -0.93298465, -1.490719  ],\n",
       "         ...,\n",
       "         [-0.42753804, -0.67154676, -0.9504138 , ..., -1.0375599 ,\n",
       "          -1.1769934 , -1.5255773 ],\n",
       "         [-0.86326784, -1.403573  , -1.6998693 , ..., -1.7347276 ,\n",
       "          -1.68244   , -1.6998693 ],\n",
       "         [-1.5081482 , -1.8044444 , -1.8044444 , ..., -1.8044444 ,\n",
       "          -1.7347276 , -1.6998693 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.predict_data(image_path=\"image/008.bathtub/008_0050.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changebio/anaconda3/envs/fastv1/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torchvision.models.densenet.DenseNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "from flyai.dataset import Dataset\n",
    "\n",
    "from model import Model\n",
    "from path import MODEL_PATH\n",
    "\n",
    "data = Dataset()\n",
    "model = Model(data)\n",
    "p = model.predict(image_path=\"image/008.bathtub/008_0050.jpg\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image/097.harmonica/097_0010.jpg\n",
      "image/145.motorbikes-101/145_0455.jpg\n",
      "image/057.dolphin-101/057_0072.jpg\n",
      "image/192.snowmobile/192_0093.jpg\n",
      "image/154.palm-tree/154_0101.jpg\n",
      "image/073.fireworks/073_0092.jpg\n",
      "image/070.fire-extinguisher/070_0023.jpg\n",
      "image/256.toad/256_0051.jpg\n",
      "image/234.tweezer/234_0020.jpg\n",
      "image/230.trilobite-101/230_0005.jpg\n",
      "image/126.ladder/126_0201.jpg\n",
      "image/177.saturn/177_0059.jpg\n",
      "image/218.tennis-racket/218_0068.jpg\n",
      "image/105.horse/105_0036.jpg\n",
      "image/106.horseshoe-crab/106_0006.jpg\n",
      "image/164.porcupine/164_0096.jpg\n",
      "image/240.watch-101/240_0038.jpg\n",
      "image/039.chopsticks/039_0038.jpg\n",
      "image/005.baseball-glove/005_0136.jpg\n",
      "image/051.cowboy-hat/051_0089.jpg\n",
      "image/096.hammock/096_0275.jpg\n",
      "image/235.umbrella-101/235_0078.jpg\n",
      "image/033.cd/033_0087.jpg\n",
      "image/175.roulette-wheel/175_0058.jpg\n",
      "image/145.motorbikes-101/145_0624.jpg\n",
      "image/132.light-house/132_0094.jpg\n",
      "image/092.grapes/092_0051.jpg\n",
      "image/212.teapot/212_0112.jpg\n",
      "image/014.blimp/014_0036.jpg\n",
      "image/214.teepee/214_0079.jpg\n",
      "image/232.t-shirt/232_0090.jpg\n",
      "image/170.rainbow/170_0084.jpg\n",
      "image/011.billiards/011_0166.jpg\n",
      "image/230.trilobite-101/230_0002.jpg\n",
      "image/055.dice/055_0024.jpg\n",
      "image/045.computer-keyboard/045_0071.jpg\n",
      "image/124.killer-whale/124_0057.jpg\n",
      "image/222.tombstone/222_0084.jpg\n",
      "image/019.boxing-glove/019_0057.jpg\n",
      "image/241.waterfall/241_0010.jpg\n",
      "image/096.hammock/096_0210.jpg\n",
      "image/144.minotaur/144_0060.jpg\n",
      "image/059.drinking-straw/059_0027.jpg\n",
      "image/087.goldfish/087_0090.jpg\n",
      "image/028.camel/028_0050.jpg\n",
      "image/250.zebra/250_0016.jpg\n",
      "image/035.cereal-box/035_0084.jpg\n",
      "image/108.hot-dog/108_0005.jpg\n",
      "image/029.cannon/029_0002.jpg\n",
      "image/014.blimp/014_0077.jpg\n",
      "image/092.grapes/092_0101.jpg\n",
      "image/090.gorilla/090_0098.jpg\n",
      "image/019.boxing-glove/019_0076.jpg\n",
      "image/122.kayak/122_0069.jpg\n",
      "image/192.snowmobile/192_0026.jpg\n",
      "image/124.killer-whale/124_0074.jpg\n",
      "image/145.motorbikes-101/145_0324.jpg\n",
      "image/166.praying-mantis/166_0081.jpg\n",
      "image/140.menorah-101/140_0069.jpg\n",
      "image/163.playing-card/163_0005.jpg\n",
      "image/123.ketch-101/123_0076.jpg\n",
      "image/050.covered-wagon/050_0012.jpg\n",
      "image/082.galaxy/082_0066.jpg\n",
      "image/232.t-shirt/232_0254.jpg\n",
      "image/181.segway/181_0016.jpg\n",
      "image/149.necktie/149_0003.jpg\n",
      "image/244.wheelbarrow/244_0025.jpg\n",
      "image/092.grapes/092_0133.jpg\n",
      "image/071.fire-hydrant/071_0009.jpg\n",
      "image/082.galaxy/082_0037.jpg\n",
      "image/236.unicorn/236_0043.jpg\n",
      "image/232.t-shirt/232_0094.jpg\n",
      "image/217.tennis-court/217_0036.jpg\n",
      "image/124.killer-whale/124_0073.jpg\n",
      "image/133.lightning/133_0030.jpg\n",
      "image/011.billiards/011_0259.jpg\n",
      "image/209.sword/209_0054.jpg\n",
      "image/184.sheet-music/184_0003.jpg\n",
      "image/156.paper-shredder/156_0064.jpg\n",
      "image/185.skateboard/185_0017.jpg\n",
      "image/150.octopus/150_0054.jpg\n",
      "image/213.teddy-bear/213_0031.jpg\n",
      "image/096.hammock/096_0220.jpg\n",
      "image/098.harp/098_0017.jpg\n",
      "image/018.bowling-pin/018_0050.jpg\n",
      "image/128.lathe/128_0058.jpg\n",
      "image/116.iguana/116_0014.jpg\n",
      "image/085.goat/085_0027.jpg\n",
      "image/080.frog/080_0013.jpg\n",
      "image/117.ipod/117_0080.jpg\n",
      "image/043.coin/043_0061.jpg\n",
      "image/242.watermelon/242_0043.jpg\n",
      "image/048.conch/048_0025.jpg\n",
      "image/008.bathtub/008_0050.jpg\n",
      "image/090.gorilla/090_0152.jpg\n",
      "image/115.ice-cream-cone/115_0007.jpg\n",
      "image/128.lathe/128_0070.jpg\n",
      "image/223.top-hat/223_0021.jpg\n",
      "image/105.horse/105_0198.jpg\n",
      "image/218.tennis-racket/218_0072.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in data.get_all_data()[0]:\n",
    "    print(i['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96,\n",
       " 144,\n",
       " 56,\n",
       " 191,\n",
       " 153,\n",
       " 72,\n",
       " 69,\n",
       " 255,\n",
       " 144,\n",
       " 229,\n",
       " 125,\n",
       " 144,\n",
       " 217,\n",
       " 104,\n",
       " 229,\n",
       " 95,\n",
       " 239,\n",
       " 144,\n",
       " 4,\n",
       " 50,\n",
       " 95,\n",
       " 95,\n",
       " 32,\n",
       " 174,\n",
       " 144,\n",
       " 131,\n",
       " 91,\n",
       " 231,\n",
       " 13,\n",
       " 95,\n",
       " 231,\n",
       " 169,\n",
       " 144,\n",
       " 229,\n",
       " 144,\n",
       " 144,\n",
       " 231,\n",
       " 95,\n",
       " 231,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 229,\n",
       " 153,\n",
       " 229,\n",
       " 144,\n",
       " 91,\n",
       " 144,\n",
       " 95,\n",
       " 231,\n",
       " 91,\n",
       " 231,\n",
       " 144,\n",
       " 95,\n",
       " 191,\n",
       " 56,\n",
       " 144,\n",
       " 91,\n",
       " 231,\n",
       " 231,\n",
       " 191,\n",
       " 231,\n",
       " 91,\n",
       " 231,\n",
       " 144,\n",
       " 144,\n",
       " 72,\n",
       " 123,\n",
       " 231,\n",
       " 69,\n",
       " 231,\n",
       " 91,\n",
       " 123,\n",
       " 123,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 231,\n",
       " 144,\n",
       " 144,\n",
       " 231,\n",
       " 95,\n",
       " 95,\n",
       " 144,\n",
       " 144,\n",
       " 95,\n",
       " 104,\n",
       " 231,\n",
       " 144,\n",
       " 231,\n",
       " 144,\n",
       " 229,\n",
       " 144,\n",
       " 95,\n",
       " 95,\n",
       " 144,\n",
       " 144,\n",
       " 191,\n",
       " 13]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.predict(image_path=i['image_path']) for i in data.get_all_data()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': 'image/097.harmonica/097_0010.jpg'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.get_all_data()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changebio/anaconda3/envs/fastv1/lib/python3.6/site-packages/torch/serialization.py:434: SourceChangeWarning: source code of class 'torchvision.models.densenet.DenseNet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "input_x() got an unexpected keyword argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9a5bd077f8c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/Hdd/Proj/github/flyai/Caltech256_FlyAI/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda_avail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastv1/lib/python3.6/site-packages/flyai/dataset.py\u001b[0m in \u001b[0;36mpredict_data\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mprocessor_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastv1/lib/python3.6/site-packages/flyai/dataset.py\u001b[0m in \u001b[0;36mget_method_dict\u001b[0;34m(self, clz, method_name, **args)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_method_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_method_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: input_x() got an unexpected keyword argument 'data'"
     ]
    }
   ],
   "source": [
    "model.predict(data=data.get_all_data()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "predict_data() argument after ** must be a mapping, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fba3544a5d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/Hdd/Proj/github/flyai/Caltech256_FlyAI/model.py\u001b[0m in \u001b[0;36mpredict_all\u001b[0;34m(self, datas)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_data() argument after ** must be a mapping, not str"
     ]
    }
   ],
   "source": [
    "model.predict_all(data.get_all_data()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'image/097.harmonica/097_0010.jpg'}\n",
      "{'image_path': 'image/145.motorbikes-101/145_0455.jpg'}\n",
      "{'image_path': 'image/057.dolphin-101/057_0072.jpg'}\n",
      "{'image_path': 'image/192.snowmobile/192_0093.jpg'}\n",
      "{'image_path': 'image/154.palm-tree/154_0101.jpg'}\n",
      "{'image_path': 'image/073.fireworks/073_0092.jpg'}\n",
      "{'image_path': 'image/070.fire-extinguisher/070_0023.jpg'}\n",
      "{'image_path': 'image/256.toad/256_0051.jpg'}\n",
      "{'image_path': 'image/234.tweezer/234_0020.jpg'}\n",
      "{'image_path': 'image/230.trilobite-101/230_0005.jpg'}\n",
      "{'image_path': 'image/126.ladder/126_0201.jpg'}\n",
      "{'image_path': 'image/177.saturn/177_0059.jpg'}\n",
      "{'image_path': 'image/218.tennis-racket/218_0068.jpg'}\n",
      "{'image_path': 'image/105.horse/105_0036.jpg'}\n",
      "{'image_path': 'image/106.horseshoe-crab/106_0006.jpg'}\n",
      "{'image_path': 'image/164.porcupine/164_0096.jpg'}\n",
      "{'image_path': 'image/240.watch-101/240_0038.jpg'}\n",
      "{'image_path': 'image/039.chopsticks/039_0038.jpg'}\n",
      "{'image_path': 'image/005.baseball-glove/005_0136.jpg'}\n",
      "{'image_path': 'image/051.cowboy-hat/051_0089.jpg'}\n",
      "{'image_path': 'image/096.hammock/096_0275.jpg'}\n",
      "{'image_path': 'image/235.umbrella-101/235_0078.jpg'}\n",
      "{'image_path': 'image/033.cd/033_0087.jpg'}\n",
      "{'image_path': 'image/175.roulette-wheel/175_0058.jpg'}\n",
      "{'image_path': 'image/145.motorbikes-101/145_0624.jpg'}\n",
      "{'image_path': 'image/132.light-house/132_0094.jpg'}\n",
      "{'image_path': 'image/092.grapes/092_0051.jpg'}\n",
      "{'image_path': 'image/212.teapot/212_0112.jpg'}\n",
      "{'image_path': 'image/014.blimp/014_0036.jpg'}\n",
      "{'image_path': 'image/214.teepee/214_0079.jpg'}\n",
      "{'image_path': 'image/232.t-shirt/232_0090.jpg'}\n",
      "{'image_path': 'image/170.rainbow/170_0084.jpg'}\n",
      "{'image_path': 'image/011.billiards/011_0166.jpg'}\n",
      "{'image_path': 'image/230.trilobite-101/230_0002.jpg'}\n",
      "{'image_path': 'image/055.dice/055_0024.jpg'}\n",
      "{'image_path': 'image/045.computer-keyboard/045_0071.jpg'}\n",
      "{'image_path': 'image/124.killer-whale/124_0057.jpg'}\n",
      "{'image_path': 'image/222.tombstone/222_0084.jpg'}\n",
      "{'image_path': 'image/019.boxing-glove/019_0057.jpg'}\n",
      "{'image_path': 'image/241.waterfall/241_0010.jpg'}\n",
      "{'image_path': 'image/096.hammock/096_0210.jpg'}\n",
      "{'image_path': 'image/144.minotaur/144_0060.jpg'}\n",
      "{'image_path': 'image/059.drinking-straw/059_0027.jpg'}\n",
      "{'image_path': 'image/087.goldfish/087_0090.jpg'}\n",
      "{'image_path': 'image/028.camel/028_0050.jpg'}\n",
      "{'image_path': 'image/250.zebra/250_0016.jpg'}\n",
      "{'image_path': 'image/035.cereal-box/035_0084.jpg'}\n",
      "{'image_path': 'image/108.hot-dog/108_0005.jpg'}\n",
      "{'image_path': 'image/029.cannon/029_0002.jpg'}\n",
      "{'image_path': 'image/014.blimp/014_0077.jpg'}\n",
      "{'image_path': 'image/092.grapes/092_0101.jpg'}\n",
      "{'image_path': 'image/090.gorilla/090_0098.jpg'}\n",
      "{'image_path': 'image/019.boxing-glove/019_0076.jpg'}\n",
      "{'image_path': 'image/122.kayak/122_0069.jpg'}\n",
      "{'image_path': 'image/192.snowmobile/192_0026.jpg'}\n",
      "{'image_path': 'image/124.killer-whale/124_0074.jpg'}\n",
      "{'image_path': 'image/145.motorbikes-101/145_0324.jpg'}\n",
      "{'image_path': 'image/166.praying-mantis/166_0081.jpg'}\n",
      "{'image_path': 'image/140.menorah-101/140_0069.jpg'}\n",
      "{'image_path': 'image/163.playing-card/163_0005.jpg'}\n",
      "{'image_path': 'image/123.ketch-101/123_0076.jpg'}\n",
      "{'image_path': 'image/050.covered-wagon/050_0012.jpg'}\n",
      "{'image_path': 'image/082.galaxy/082_0066.jpg'}\n",
      "{'image_path': 'image/232.t-shirt/232_0254.jpg'}\n",
      "{'image_path': 'image/181.segway/181_0016.jpg'}\n",
      "{'image_path': 'image/149.necktie/149_0003.jpg'}\n",
      "{'image_path': 'image/244.wheelbarrow/244_0025.jpg'}\n",
      "{'image_path': 'image/092.grapes/092_0133.jpg'}\n",
      "{'image_path': 'image/071.fire-hydrant/071_0009.jpg'}\n",
      "{'image_path': 'image/082.galaxy/082_0037.jpg'}\n",
      "{'image_path': 'image/236.unicorn/236_0043.jpg'}\n",
      "{'image_path': 'image/232.t-shirt/232_0094.jpg'}\n",
      "{'image_path': 'image/217.tennis-court/217_0036.jpg'}\n",
      "{'image_path': 'image/124.killer-whale/124_0073.jpg'}\n",
      "{'image_path': 'image/133.lightning/133_0030.jpg'}\n",
      "{'image_path': 'image/011.billiards/011_0259.jpg'}\n",
      "{'image_path': 'image/209.sword/209_0054.jpg'}\n",
      "{'image_path': 'image/184.sheet-music/184_0003.jpg'}\n",
      "{'image_path': 'image/156.paper-shredder/156_0064.jpg'}\n",
      "{'image_path': 'image/185.skateboard/185_0017.jpg'}\n",
      "{'image_path': 'image/150.octopus/150_0054.jpg'}\n",
      "{'image_path': 'image/213.teddy-bear/213_0031.jpg'}\n",
      "{'image_path': 'image/096.hammock/096_0220.jpg'}\n",
      "{'image_path': 'image/098.harp/098_0017.jpg'}\n",
      "{'image_path': 'image/018.bowling-pin/018_0050.jpg'}\n",
      "{'image_path': 'image/128.lathe/128_0058.jpg'}\n",
      "{'image_path': 'image/116.iguana/116_0014.jpg'}\n",
      "{'image_path': 'image/085.goat/085_0027.jpg'}\n",
      "{'image_path': 'image/080.frog/080_0013.jpg'}\n",
      "{'image_path': 'image/117.ipod/117_0080.jpg'}\n",
      "{'image_path': 'image/043.coin/043_0061.jpg'}\n",
      "{'image_path': 'image/242.watermelon/242_0043.jpg'}\n",
      "{'image_path': 'image/048.conch/048_0025.jpg'}\n",
      "{'image_path': 'image/008.bathtub/008_0050.jpg'}\n",
      "{'image_path': 'image/090.gorilla/090_0152.jpg'}\n",
      "{'image_path': 'image/115.ice-cream-cone/115_0007.jpg'}\n",
      "{'image_path': 'image/128.lathe/128_0070.jpg'}\n",
      "{'image_path': 'image/223.top-hat/223_0021.jpg'}\n",
      "{'image_path': 'image/105.horse/105_0198.jpg'}\n",
      "{'image_path': 'image/218.tennis-racket/218_0072.jpg'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[96,\n",
       " 144,\n",
       " 56,\n",
       " 191,\n",
       " 153,\n",
       " 72,\n",
       " 69,\n",
       " 255,\n",
       " 144,\n",
       " 229,\n",
       " 125,\n",
       " 144,\n",
       " 217,\n",
       " 104,\n",
       " 229,\n",
       " 95,\n",
       " 239,\n",
       " 144,\n",
       " 4,\n",
       " 50,\n",
       " 95,\n",
       " 95,\n",
       " 32,\n",
       " 174,\n",
       " 144,\n",
       " 131,\n",
       " 91,\n",
       " 231,\n",
       " 13,\n",
       " 95,\n",
       " 231,\n",
       " 169,\n",
       " 144,\n",
       " 229,\n",
       " 144,\n",
       " 144,\n",
       " 231,\n",
       " 95,\n",
       " 231,\n",
       " 95,\n",
       " 95,\n",
       " 95,\n",
       " 229,\n",
       " 153,\n",
       " 229,\n",
       " 144,\n",
       " 91,\n",
       " 144,\n",
       " 95,\n",
       " 231,\n",
       " 91,\n",
       " 231,\n",
       " 144,\n",
       " 95,\n",
       " 191,\n",
       " 56,\n",
       " 144,\n",
       " 91,\n",
       " 231,\n",
       " 231,\n",
       " 191,\n",
       " 231,\n",
       " 91,\n",
       " 231,\n",
       " 144,\n",
       " 144,\n",
       " 72,\n",
       " 123,\n",
       " 231,\n",
       " 69,\n",
       " 231,\n",
       " 91,\n",
       " 123,\n",
       " 123,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 144,\n",
       " 231,\n",
       " 144,\n",
       " 144,\n",
       " 231,\n",
       " 95,\n",
       " 95,\n",
       " 144,\n",
       " 144,\n",
       " 95,\n",
       " 104,\n",
       " 231,\n",
       " 144,\n",
       " 231,\n",
       " 144,\n",
       " 229,\n",
       " 144,\n",
       " 95,\n",
       " 95,\n",
       " 144,\n",
       " 144,\n",
       " 191,\n",
       " 13]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_all(data.get_all_data()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_data() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e155e3cdd4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict_data() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "data.predict_data(data.get_all_data()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "??model.predict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x flyai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码提交中请等待 ...\n",
      "离线训练提交成功！复制链接到浏览器，登录后即可查看实时训练日志。\n",
      "https://flyai.com/training_result/taind1b41b6d26a5ec054b54437f\n"
     ]
    }
   ],
   "source": [
    "! ./flyai train -p=1 -b=32 -e=100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from flyai.dataset import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "from model import Model\n",
    "from path import MODEL_PATH\n",
    "\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "# 数据获取辅助类\n",
    "dataset = Dataset()\n",
    "\n",
    "# 模型操作辅助类\n",
    "model = Model(dataset)\n",
    "\n",
    "# 超参\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--EPOCHS\", default=10, type=int, help=\"train epochs\")\n",
    "parser.add_argument(\"-b\", \"--BATCH\", default=1, type=int, help=\"batch size\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def eval(model, x_test, y_test):\n",
    "    cnn.eval()\n",
    "    batch_eval = model.batch_iter(x_test, y_test)\n",
    "    total_acc = 0.0\n",
    "    data_len = len(x_test)\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        outputs = cnn(x_batch)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        correct = (prediction == y_batch).sum().item()\n",
    "        acc = correct / batch_len\n",
    "        total_acc += acc * batch_len\n",
    "    return total_acc / data_len\n",
    "\n",
    "\n",
    "## 定义DenseNet实例,加载与训练模型，并更改最后一层\n",
    "cnn = torchvision.models.densenet121(pretrained=True)\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "num_features = cnn.classifier.in_features\n",
    "cnn.classifier = nn.Linear(num_features, 256)\n",
    "\n",
    "if cuda_avail:\n",
    "    cnn.cuda()\n",
    "\n",
    "optimizer = Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.999))  # 选用AdamOptimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # 定义损失函数\n",
    "\n",
    "# 训练并评估模型\n",
    "\n",
    "data = Dataset()\n",
    "model = Model(data)\n",
    "\n",
    "best_accuracy = 0\n",
    "for i in range(args.EPOCHS):\n",
    "    cnn.train()\n",
    "    x_train, y_train, x_test, y_test = data.next_batch(args.BATCH)  # 读取数据\n",
    "\n",
    "    x_train = torch.from_numpy(x_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    x_train = x_train.float()\n",
    "\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "    x_test = x_test.float()\n",
    "\n",
    "    if cuda_avail:\n",
    "        x_train = Variable(x_train.cuda())\n",
    "        y_train = Variable(y_train.cuda())\n",
    "        x_test = Variable(x_test.cuda())\n",
    "        y_test = Variable(y_test.cuda())\n",
    "\n",
    "    outputs = cnn(x_train)\n",
    "    _, prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_fn(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 若测试准确率高于当前最高准确率，则保存模型\n",
    "    train_accuracy = eval(model, x_test, y_test)\n",
    "    if train_accuracy > best_accuracy:\n",
    "        best_accuracy = train_accuracy\n",
    "        model.save_model(cnn, MODEL_PATH, overwrite=True)\n",
    "        print(\"step %d, best accuracy %g\" % (i, best_accuracy))\n",
    "\n",
    "    print(str(i) + \"/\" + str(args.EPOCHS))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
