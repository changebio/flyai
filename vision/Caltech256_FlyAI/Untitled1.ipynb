{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "# author Huangyin\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from flyai.dataset import Dataset\n",
    "from flyai.source.base import DATA_PATH\n",
    "\n",
    "from model import Model\n",
    "from path import MODEL_PATH\n",
    "from processor import Caltech256\n",
    "from utils import Bunch\n",
    "    \n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    \n",
    "def pretrained_net(net,class_num):\n",
    "    if net=='densenet121':\n",
    "        cnn = torchvision.models.densenet121(pretrained=True)\n",
    "        for param in cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_features = cnn.classifier.in_features\n",
    "        cnn.classifier = nn.Linear(num_features, class_num)\n",
    "    elif net=='resnet34':\n",
    "        cnn = torchvision.models.resnet34(pretrained=True)\n",
    "        for param in cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_features = cnn.fc.in_features\n",
    "        cnn.fc = nn.Sequential(nn.Linear(num_features, class_num),nn.Sigmoid())\n",
    "    return cnn\n",
    "\n",
    "def score(p,y):\n",
    "    _,yp = torch.max(p.data,1)\n",
    "    return (yp == y).sum().item()/len(y)\n",
    "\n",
    "\n",
    "    \n",
    "def eval(model, x_test, y_test):\n",
    "    net.eval()\n",
    "    batch_eval = model.batch_iter(x_test, y_test)\n",
    "    total_acc = 0.0\n",
    "    data_len = len(x_test)\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        outputs = net(x_batch)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        correct = (prediction == y_batch).sum().item()\n",
    "        acc = correct / batch_len\n",
    "        total_acc += acc * batch_len\n",
    "    return total_acc / data_len\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-e\", \"--EPOCHS\", default=10, type=int, help=\"train epochs\")\n",
    "    parser.add_argument(\"-b\", \"--BATCH\", default=32, type=int, help=\"batch size\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "  \n",
    "    #settings\n",
    "    settings = {\n",
    "    'net':'densenet121',\n",
    "    'nc':256,    \n",
    "    'lr': 0.001,\n",
    "    'seed': 1,\n",
    "    'log_interval': 10,\n",
    "    'save_model': True}\n",
    "    print(\"1.settings\",settings)\n",
    "    settings = Bunch(settings)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(settings.seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    #load data\n",
    "    data = Dataset()\n",
    "    model = Model(data)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    train_transforms= transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    train_dataset = Caltech256(root=DATA_PATH,df=data.db.source.data,transform=train_transforms)\n",
    "    val_dataset = Caltech256(root=DATA_PATH,df=data.db.source.test,transform=val_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "        batch_size=args.BATCH, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "        batch_size=args.BATCH, shuffle=True, **kwargs)\n",
    "    n_train = len(train_dataset)\n",
    "    batch_train = n_train/args.BATCH\n",
    "    n_test = len(val_dataset)\n",
    "    batch_test = n_test/args.BATCH\n",
    "    print(\"2. load data. train_dataset %d,batch %d, val_dataset %d, batch %d.\" % (n_train,batch_train,n_test,batch_test))\n",
    "\n",
    "    #load net structure\n",
    "    print(\"3.load net structure: %s, number of class: %d\" % (settings.net,settings.nc))\n",
    "    net = pretrained_net(settings.net,settings.nc)\n",
    "    net = pretrained_net('densenet121',256)\n",
    "    gpu = torch.cuda.is_available()\n",
    "    if gpu:\n",
    "        net.cuda()\n",
    "    #optimize and loss\n",
    "    print(\"4.optimize and loss. learning rate %g\" % settings.lr)\n",
    "    optimizer = Adam(net.parameters(), lr=settings.lr, betas=(0.9, 0.999)) \n",
    "    #optimizer = Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999))  \n",
    "    loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "    #train and test\n",
    "    print(\"5.***************train and test*********************\")\n",
    "    best_accuracy = 0\n",
    "    train_iter = iter(train_loader)\n",
    "    batch_idx = 0\n",
    "    for i in range(args.EPOCHS):\n",
    "        net.train()\n",
    "        #x_train, y_train, x_test, y_test = data.next_batch(args.BATCH) \n",
    "        #x_train = torch.from_numpy(x_train)\n",
    "        #y_train = torch.from_numpy(y_train)\n",
    "        #x_train = x_train.float()\n",
    "        try:\n",
    "            batch_idx +=1\n",
    "            x_train, y_train = next(train_iter)\n",
    "            #print(batch_idx,\"data len\",len(x_train),len(y_train))\n",
    "        except:\n",
    "            batch_idx = 0\n",
    "            train_iter = iter(train_loader)\n",
    "            x_train, y_train = next(train_iter)\n",
    "            print(len(x_train),len(y_train),\"data len\")\n",
    "\n",
    "        if gpu:\n",
    "            x_train = Variable(x_train.cuda())\n",
    "            y_train = Variable(y_train.cuda())\n",
    "\n",
    "\n",
    "        outputs = net(x_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % settings.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                i, batch_idx * len(x_train), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        if i % 10 == 0:\n",
    "            print(\"batch: %d,train loss: %g, train score: %g\" % (batch_idx,loss.data.item(), score(outputs,y_train)))\n",
    "            \n",
    "    if settings.save_model:       \n",
    "        model.save_model(net, MODEL_PATH, overwrite=True)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
